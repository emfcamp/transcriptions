>> You see, I am gonna have to ask you to do that again, because that wasn't quite enough whooping for me. I'm gonna say how are you all doing? And you're gonna give a big EMF whoop. Good evening, everyone! How are you doing? You're wonderful. You're so much better than the last audience we had in here. So just before we start, I have a couple of messages. So we are still on the lookout for volunteers. There are shifts available in the bar this evening. So if you want to get served a drink quickly, volunteer for that. There's plenty of spaces tomorrow for people to do AV and talking up on here. All sorts of things. At 10:30 tomorrow morning, the satellites are gonna be passing over us, taking some images. If you want to be part of a giant art installation, head over to the null sector or the shipping containers, where it's a bit Blade Runnery. And we're gonna do a big art installation. And at 7:00 pm tonight, outside the bar, is Hebocon, which is crappy Robot Wars, I've been told. If you're interested in battling some robots, go sign up. If you want to see some crappy robots fighting, head to the bar. And if you go to the sidebar tonight, there's a tech swap. Bring your unwanted tech. Leave with someone else's unwanted tech. It's a glorious way to recycle.

But if we are ready... We're not quite ready? I'm gonna... Just waiting for some sound issues. I will tell you some other interesting things. Feedback! So we really hope that you're enjoying EMF. Are you enjoying EMF? Well, what would be lovely is if you went to EMF Camp.org/feedback, tell the organizers either how much you're enjoying it, or give them some constructive criticism about how EMF camp 2020 can be better. That would be just swell. Because we really need to know is what you like, what you don't like. It can be completely anonymous, if you want, or you can leave your praise with your full name and address, and then Jonty can come find you and shake your hand. So we are very nearly ready. So... Coming up next, just to let you know, you are in Tent... Oh, Tent C? I had no idea.

This is my favorite tent of all of them. Don't tell Tent A. It'll get jealous. But Tent A... Where you are going to be seeing an amazing talk by Barney Livingston, talking about his Poetoid Lyricam. Have I pronounced that right? Close enough. I need you to start clapping, but very, very gently. Can you start clapping very gently? Bring it up a bit. A bit more. A bit more. Please welcome Poetoid Lyricam!

BARNEY: Thank you! My name is Barney. I would like to talk to you about this. Poetoid Lyricam thing that I made. Before I start, I'd like to give a disclaimer, in case there are any real poets in the room. What this produces is probably best described as prose poetry or free verse or maybe bad poetry. If there are any actual poets, I am quite interested in collaborating to improve the poeticalness of it. Okay. So... I'll start with a demonstration. I'm going to take a poem of you all. In case you're worried about being photographed, the actual image is only kept briefly and is then thrown away, and there's no network or anything. If you're still worried, then hide your face now. You have to wait for it to develop. I have no idea what it's going to say. I apologize if it's less than complimentary. Okay. The city. A group of people sitting on a bench. Many images of people on a city street. People are sitting. While standing in a street under flags. City standing kids. The woman is talking near other kids, standing in the street. Kids.

(applause)

Okay. Have that work. Inside is a Raspberry Pi 3B+. There's a Pi camera in the front, a nanothermal printer from AdaFruit, two 18650 lithium batteries, in series, to make about 8 volts, there's a Pimoroni wide input shim, attached to the Pi, and so the printer wants 5 to 9 volts and the Pi and shim wants 3 to 16 volts, so they both work within the range of the batteries. The batteries tend to last more than a day, I find. So for the camera itself... It was originally called Super Colour Swinger by Polaroid. Made in 1975. Pretty terrible camera. And the film is no longer made, so I don't feel terrible about gutting it. It has the advantage of having a lot of space inside, and I was able to keep the shutter button working. And it has a handy locking function. This is the third version of this. The first version I made for EMF in 2016.

It used to send the poem to a server in my house via the Wi-Fi. And if you remember, EMF in 2016, the Wi-Fi was not very reliable. So the camera wasn't very reliable. I was determined to make it entirely self-contained, which I have done. The second iteration was a bit... Much too slow. It took close to a minute for the poem to come out. But I have spent quite a lot of time fixing that. And as you saw, it's reasonably quicker now. So before I talk about the intricacies of the software, I would like to go back and have a look at the history of how this project came about. In 1916, during the First World War, a group of artist refugees gathered in Zurich. Their reaction to the horror going all around them was to abandon the usual forms of art and embrace the absurd. They chose a name for their group by selecting a word at random from a French-German dictionary. That word was Dada. A prominent member of the group was the Romanian poet Tristan Zara. The Dadaists wrote a lot of manifestos. In 1920, he wrote a manifesto on feeble love and bitter love in which he gave instructions on how to make a Dadaist poem. To make a Dadaist poem, take a newspaper.

Take some scissors. Choose from the paper an article of length you want to make your poem. Cut out the article. Next carefully cut out each of the words that makes this article and put them all in a bag. Shake gently. Next, take out each cutting, one after the other. Copy conscientiously in the order in the bag. And there you are. An infinitely original author of sensibility unappreciated by the vulgar herd. This became known the cut-up technique. Several people across literature and music have employed the cut-up technique. William S Burroughs used it in several of his books. Thom Yorke from Radiohead used it in his Kid A album, and also this chap.

>> When we were in Los Angeles in '74, you were still using that technique of cut-ups. Do you still use it?

>> Yeah. Yeah. Increasingly so. To a great extent on Outside. Even on the new album, Earthling. If you put three or four dissociated ideas together, and created awkward relationships with them, the unconscious intelligence that comes from that, those pairings, is really quite startling sometimes. Quite... Quite... Quite provocative. A friend of mine in San Francisco developed a program for me on the computer. Which enables me to do it really quickly.

>> So instead of going through the laborious process of cutting things up... You use your computer.

>> Yeah, and you can work with far more material. So I'll take articles out of newspapers, poems that I've written, pieces of other people's books. And put them all into this little warehouse of... This container of information. And then hit the random button. And it'll randomize everything. And I'll get reams of papers back out of it. With interesting ideas. And then I'll either take sentences verbatim, as it spews them out, or there might be something within a sentence which triggers off an idea.

>> I love how pleased he is with his program. I think I would have liked to have been David Bowie's software developer. Around all this... During all this time, I was born. Growing up, I suppose the influence of the Dadaists came to me through this country's tradition of absurdist comedy. Spike Milligan, Monty Python, all that lot. My first experiment in generating text came through the form of Mrs. Hathaway's Knickers. Invented by my grandfather. A child was given a piece of paper with a name written on it. The adult would start reading a book, and the child would read that word. For example, Mrs. Hathaway's Knickers. Mrs. Hathaway being one of my mother's teachers. This was hilarious. Like many here, I learned to program on a BBC Micro. In fact, the exact BBC Micro is over at the bar showing Twitter at the moment. When I was 13, my friend Peter Jones -- not that one -- and I collaborated on a program called Ode. I'll demonstrate it now.

(grinding noises)

>> So as you can see, the first line is based on the title of a poem by the poet master Grunthos the Flatulent, the second worst poet in galaxy, from the section on Vogon Poetry in Douglas Adam's Hitchhiker's Guide. This is a more sophisticated version of Mrs. Hathaway's Knickers. It consists of a large amount of spaghetti code, and you can see some bugs sometimes, but at its heart, there are two lines. The two lines are templates with place holder variables that are replaced by words or chunks of texts selected at random from lists of the appropriate type. This is similar to the game Mad Libs. With the right choice of templates and carefully crafted lists, this technique can be used to produce some interesting results. Much better than Ode, certainly.

FactBot1 is a Twitter bot based on the idea that people on Twitter will believe generally plausible facts. This dates back before the current fake news disaster. You can hear more about this, shortly after this talk, in Stage B. Frequently it's not quite as plausible, though. Yoko Ono Bot by Rob Manuel combines templates based on Yoko Ono's tweets combined with lists of things like foods, computers, celebrities. Tracery is a JavaScript library written by Kate Compton which uses grammars described in JSON. Which produce text in a similar way. So these words are replaced by items from these lists, and the results come out like that. This is used by a site called cheap bots done quick.com, that allows anyone to make Twitter bots easily. And many have. And I think they tend to make Twitter slightly less horrible than it is, generally. These are a couple of my favorites. This is a kind of a dungeons and dragons, based in IKEA. There are some tasty bargains.

Back to stuff I made. Markov chains. I first discovered these when playing with dissociated text program built into the text editor emacs. Which is very strange in itself, as I'm a Vi user. Markov changes are built from texts which are cut up, and then a graph is built, we encodes the likelihood that a word will be followed by another word. The graph can be traversed randomly to build a new text similar to the original and usually somewhat grammatically correct. It's probably easier if I demonstrate this. So at the top is a sentence that we're going to take apart and turn into a graph. So we start with A. And so we have an edge going to A. And then we take Markov. And that comes off A. And chain. Is. And then from is we go back to A. And stochastic from A, model, and then... Stop. That's the end of the sentence. Okay. And another sentence. So same thing. We start at the start node. Then we have Andrey Markov. We already had Markov. So we go back to there. Was. Here. A again. Back to the A node. It's a popular node. Russian mathematician. And stop. Okay. Final sentence. A cat is a furry mammal. So what happens this time... Start. Then to A. But because you've already got A, you already have from start to A, so you updated the edge to two. And then cat comes off A. And then is. And then we already have an edge from is to A, so that becomes two. And then furry mammal. And stop. Okay. So now we have a complete graph of our text. I can show you what happens if we traverse it. So we start at start. Then, for instance, it's quite likely that we'll go to A, because it's 2 versus 1. A. Then we can say... Go to cat. And then we can go to is. Then we have to go back to A. And then we can go anywhere we like. So we can say... Stochastic model. A cat is a stochastic model. There we go. Do it again. Start. This time maybe we'll go to Andrey Markov was a furry mammal. And so on.

Oops. Undefined. Yeah. That's how Markov chains work for generating text. Generally the larger the body of text you feed it, the better the results, because you'll have more variety. Sam R Cosgrave is a Twitter bot I made based on Markov chains. What it does is it regularly searches Twitter for the hashtag #haiku and records what it finds. It's been running for years now and has accumulated close to 100,000 haikus. It uses these to build three Markov chains. One per line. Here is another example of its output here. Okay. All right. Inspire Ration is another bot I made. This one feeds on thousands of inspirational quote-type texts. Markov chain text tends to be quite incoherent, but sometimes it will spit out the occasional gem.

Eventually I got bored with Markov chains and moved on to AI. I started playing with character recurrent neural networks, char-rnns, in the Torch framework. These can be used in kind of a similar way to Markov chains, in that you feed them a large body of text and then they learn how to produce text that looks somewhat like the original. So I had a laugh with that, and produced some bizarre new episodes of Friends. Then I found NeuralTalk2 model. This is a neural network that takes an image as input and produces captions. It's trained on the MS Coco dataset, which is about 300,000 images with five captions each. This is what I used to create the original Lyricam. I noticed that the captions, when I was playing with it, had a vaguely poetic feeling. In November, 2016, I took part in the NaNoGenMo, national novel generating month. Similar to NaNoWriMo, but the idea was to spend a month generating a novel of 50,000 words. This is the result. AI AI. An AI's take on the film AI. I extracted 50,000 stills from the movie, generated captions, generated chapter tiles of the most common five or more letter word from the chapter that's not already been a chapter title. Finally I formatted it all through LateX. Then my friend Libby was kind enough to get one printed for me. I'll read you a sample. Chapter 9. Picture the nameless are located on the slippery side of the indiscernible. A man is taking a picture of a mirror. Lighted man with bike looking at a window and a man is taking a picture of himself in a mirror. There is a man in a restaurant using a phone. A woman is holding a cat in a kitchen. I'm afraid you can't get much sense of the film from the book. But it occasionally talks about children and Teddy bears, which I gather is in the film. I've not actually seen it. So anyway, back to the camera.

Torch doesn't work on Raspberry Pis, so I had to switch to using TensorFlow. Again, this is... Oh, and the im2txt model, which is the same as NeuralTalk2, and similarly trained on the MSCoCo dataset. When the shutter is pressed, an image is captured and stored in a Python array, passed to the neural net, and four captions are generated. NeuralTalk2 had an option called temperature which you could use to adjust the sensibleness of the output. I generally had it turned down to quite insensible. im2txt is more sensible in that it tries to create the most sensible captions possible, so I had to dumb down the source code and introduce a random element. Captions are then cleaned up and fed through the mtagger library, which tags each image with a part of speech, nouns, prepositions, et cetera. Then there's a random chance that it will break the line on a verb, a conjunction, like and or or, or preposition, like for or of. This kind of makes a vaguely poem-shaped block of text. Sometimes it will repeat some of the nouns in list form. Finally, the results go to the printer. So that's it. As I said at the start, I'd love to improve the quality of the poetry. And I would be interested in collaborating with people on that. There's a lot of scope for employing various techniques for transforming the captions.

I would also like to investigate changing the neural net to be more poetic. The only reason I haven't looked into this is that on the hardware I've got, training the model from scratch would probably take weeks. So I'll be wandering around for the rest of the event. With my poet's hat on. Taking poems. If you would like one. Please ask. These are my contact details. I also should thank my employer, Lobster Pictures, for being understanding while I prepared this talk, and prepared for EMF. Thank you.

(applause)

>> Does anyone have any questions for our speaker?

>> Can you take a selfie?

BARNEY: A selfie? Okay. I have to wait for it to develop. Anyone else have a question? I can read it afterwards.

>> Hi. How big is the database of images and texts that you use in the Raspberry Pi?

BARNEY: Well, the original database of images was 300,000 or so. But the model that it produces... I think the file, the check point file, is about 180 megabytes or something like that. Which fits quite comfortably in RAM. It takes about a minute to load off the SD card, though. Okay. You want to hear this? Ode to the two glasses of wine. An older fellow is wearing green. And white. Looks like a tie. A man wearing a tie and a hat. Looks at their teeth for the evening. An older man wearing a yellow shirt taking two glasses of wine, he holding a glass. Hair.

(applause)

>> Any other questions? Well, then I think on that lovely note, let's have another round of applause for our speaker.

(applause)

And just a reminder that EMF is put on entirely by volunteers. So if you would like to help make EMF even more awesome, you can do this by going to one of the volunteer tents or signing up online, and there are tons and tons...

