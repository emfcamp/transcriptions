>> Hey, everyone. It's time for our next talk, computational creativity with Matthew Ireland here in Tent C. But before that, just a couple short announcements. Tomorrow morning, at 10:30 in the null space, people will be gathering to form an image that will be captured by a passing satellite. Hebocon, the crappy robot contest, has started, and you can catch a bit of that after this talk. And the swap shop is in the CyBar tonight. Bring your unwanted tech stuff. Leave with someone else's. We want your feedback, and you can always give it, including anonymously, if you would like, at EMFCamp.org/feedback, and as always, EMF is put together entirely by volunteers. This event happens and is awesome and amazing and incredible because lots and lots of people choose to make it so. And you have the opportunity to be part of that effort. You can go to the volunteer tent, or you can sign up online, and make EMF super, super, double, extra awesome. And we would love to have you. It's lots of fun. You get to hold a microphone sometimes.

So... Without further ado, here is our speaker!

MATTHEW: Thanks very much. So the title of the talk is computational creativity. So two years ago, at EMF, I gave a talk looking at how we can use some ideas from the human brain, and the human thought process, in order to inspire and drive models of computation in a slightly formal sense. This talk is not like that at all. In that I'm very informally looking at the sorts of algorithms and processors that we can go through, in order to emulate some human-style creativity in an algorithmic system. So this isn't my field. This is mostly just me pointing at things and going... Isn't this cool? And hopefully you'll agree with me. So what is creativity? Psychologists have gone to great lengths to define the different kinds of creativity and narrow down, really, the essence of what we're trying to get at. I'm going to be very intuitive about it.

I'm very informal. I'm not going to do any of that. I'm just going to say: If I give you an expert in the field, if I give you an expert in music, if I give you an expert in art, if I give you an expert in literature, could they tell the difference between something that a computer has produced and something that a human has produced? So can the computer step in and take the place of Mozart or Bach or Beethoven or Shakespeare, Picasso? And that's sort of the goal we're working towards. We're nowhere near that stage yet. But maybe we'll get some ideas about how to get that. So can computers be creative? Well, we can tell computers what to do. We can tell them to add two numbers. We can write simple computer programs to do this. A human could do this too.

Computers can also do much more complicated calculations. This is a simple C program to calculate the prime factors of a number n. So a computer could do this much more quickly than a human could. And of course we can make better computers and do those calculations faster. So this is the quantum part of Shaw's algorithm. We might be able to build a quantum computer to do this, and factorize numbers very quickly. But it's a better computer. It's much faster at some tasks. But it's still not creative. The human who invented this algorithm and came up with how to do this on a quantum computer -- he was very creative. But the computer is just following a very algorithmic process. So why is this problem difficult? Sort of... Humans are creative without thinking about it.

In fact, we can't describe how we're creative. We can't describe the thought process that we go through, in order to be creative. And we'll see that that's part of the problem. So what's fundamentally different about what's going on in a computer and what's going on in a human brain? Well, in many respects, and I've enumerated some of them here, the processes are exactly the opposite. So in a computer, every operation happens synchronously on the edge of a clock. There's some global synchronizing edge of a signal that's keeping everything in order. In contrast, the human brain is highly asynchronous. It's self-timed. Different parts of the brain can push data to other components in the brain, and they're not constrained to take place on edges of the clock.

In a computer, all the representation in an electronic computer -- all data is -- the format of all the data is specified very precisely. So we specify how the data needs to be stored. What format the data is in, in order to be processed. In the brain, the encoding is highly adaptive. And will adapt to the kind of processing that's going on, and what the requirements are at the time. Electronic computers are much faster than human brains, in a way. Sort of looking at gigahertz clocks. Compare that to a 200 hertz, 300 hertz refresh time of a neuron. Modern digital computers are all digital. Brains use a mixture of analog and digital processing. One thing we might be getting right is that both systems are massively parallel. So we're building systems with more and more cores. They're doing more and more things at once. And that's a lot like the human brain, which is also highly parallel. Unfortunately, computers are somewhat less well connected.

So we're restricted in the electronic systems -- in pretty much 2D networks of circuits. Whereas a human brain is very highly interconnected. 10 or 11 neurons in the brain connected to many other neurons themselves. Also, a computer, Intel or whoever makes your processor, have a master blueprint from which they fabricate all of these processes. In the brain, there's no such master blueprint, and the connectivity can differ from one person to the next. Computation in an electronic computer is highly deterministic. Whereas in a brain, it's highly stochastic. Highly randomized. And that's very important, as we might see. So why is the problem of creativity difficult? Well, one aspect is this stochasticity and probabilistic computation. Von Neumann suggested that these random processors in the brain -- the random interarrival times of pulses on neurons -- might be the very mechanism for human thought itself. And one hypothesis for what might be going on with this probabilistic computation is: Humans, whenever they're faced with a new piece of data, whenever I'm looking at a new image, something in my brain is generating a large number of hypotheses for what this image could be.

And then I'm sort of entertaining all of these different hypotheses, generating them almost at random and exploring a very large search space of possible hypotheses, and gradually narrowing down what it could be, until I arrive at a conclusion. And the conclusion might not always be correct. Optical illusions exist. It's easy to fool my brain into thinking that something's true that's actually not. And that might be a by-product of this large search process, perhaps. So we've said that computers are very good at tasks for which we have a large degree of cognitive penetrance. So if I can describe how to do a task, I can tell you exactly how to integrate a function like X squared. Add one to the power, divide by the new power. And a computer can do that very efficiently. Computers are very good at evaluating large integrals. Much better than humans, in fact. But I can't describe how I might write down a tune that comes into my head. I can't describe how I might write a poem, to carry on from the previous talk. And these are tasks that a computer is much less good at.

So before we go on, it's worth saying as well that maybe the human thought process isn't the only way of being creative. Maybe there are better ways of being creative than the process that humans go through. Humans... The human creative process is kind of the goal of this area. So if we start there, look at the system that we have, that does exactly what we want it to, and maybe get some inspiration from it, perhaps that's a good place to start. So how are we going to start? How can we go about emulating human thought? The overarching principle here is that we want to keep things as general as possible until as late as possible. So the human brain is very good at a huge variety of tasks. It's good at composing music. It's good at writing literature. It's good at painting artistic pictures.

So we don't want to specialize into one application domain too early. We're going to design a general algorithm that can sort inputs into classes, if you like. It can do object recognition. If I give you an image of an object, it can tell you what that image is. If I give you a piece of music, it can tell you what each note in the piece is. And then I'm going to somehow try and invert this process. Do the opposite. So if I give you a description of an object, can you paint me a picture of that object? If I give you a note in a musical piece, can you generate that note? Or if I give you a genre of music, even, can you generate a sample in that genre of music? There are a few different approaches to how to do that. And it largely depends on how we designed the algorithm in the first step.

But let's go on and see a few examples of how this might work. So we'll use neural networks as our starting point. Neural networks are by no means the only algorithm that we can use to be creative. Neural networks... People get excited about them, because they've had very good results. In a large number of applications. But they're by no means the only way to move forward in this area. They're just an algorithm like any other. They're a very good algorithm at doing classification and some other tasks. But they're not the only way of doing things. We're going to take some inspiration from how the human brain works. In that we're going to exploit the highly connected aspect of it. Computers -- we've said the circuitry in them isn't highly connected. But we can simulate this high degree of connectivity in software at a higher level, if you like, by implementing neural networks as an algorithm on top of our traditional computing hardware.

Different kinds of neural networks are suited to different tasks, so we'll look at some simple neural network architectures. And then we'll extend them. So that... Adding some knowledge from application areas in order to make them more effective in those applications. Convolution on neural nets, image processing, recurrent neural nets for looking at sequences. Fundamentally, a neural network is a supervised learning algorithm. That means we have to give it a number of training examples. So we tell it what the correct answer is to a number of classification problems. And from that, it learns how to do further classifications. So we go through this training phase. We sit in the lab and wait for our neural network to train. It might take many days, many weeks, on modern neural networks like WaveNet. Many months. And then once it's trained, we've got a very efficient process for classifying new inputs.

So the training phase might take a while, but then I can put my neural network out in the open, and just give it a piece of data, and it'll classify that data as needed. Neural networks are very good when there's lots of training data available. This is a general pattern in lots of applications. And if we don't have lots of training data, then perhaps neural networks aren't the right approach. Or perhaps we need to go through some process to try and get more training data. And we'll look at some ways we can do that. The simplest building block, if you like, of a neural network, and sort of a precursor on neural networks, is the Perceptron. People tried to build these in hardware back in the '60s and '70s. Now we try to use more than one of them and connect them into a larger neural network. So the key idea... Can you see if I point with my mouse? Is that showing up on the screen? Yeah, cool.

The idea is I have a load of data items. This is my vector, X, if you like. This describes features in my input. Then I've got some linear operation. That does some preprocessing on this data input. So it multiplies each input by an associated weight. And adds them together. And then I've got this non-linear decision making operation that compares the result of this to some threshold, and then outputs a decision. So this function might output one. If the result of this linear operation is above the threshold. Or zero, otherwise. And the goal of the Perceptron, during the training phase, is to learn this function F. So once I've learned this function F from a series of labeled examples, I can then go and make decisions using it.

So we've got the two phases. Training and then application. I'll just go through a really simple example so we can see -- just get an intuition for how this works. My favorite example is the pet-aware household intruder detection system. We've got infrared sensors and weight pads. Give me the height, weight, speed, and color of the fur or clothes of the human or pet. And the goal is that it should set off an alarm if a human is trying to enter a property, but it should ignore pets. So it should output whether the person picked up by the intrusion detection system is a pet or a human. So I need to start off with a load of labeled examples.

So I have a number of examples from adults and a number of examples from pets. Then I put all this into the training algorithm. In practice, if you're doing AI, this is just an API call. There are really, really good APIs for doing machine learning. TensorFlow, Keras, and we don't have to understand how this training algorithm works. It might be interesting to learn how it works, but there are lots of people out in the real world using neural nets who just treat this training process as an API call. So that's what we'll do here. And then the output from this training algorithm is a series of weights. That I can then use to classify further examples. Those weights go on the arcs here. And then I can feed in new examples. New examples with height, weight, speed, and fur color.

And then I've got this system that will tell me whether the inputs correspond to a pet or a human. So a slightly more abstract example next. Just to develop the formalism. So here I've just got... I've got a number of examples in this 2D space. And I want to separate the orange examples from the blue examples. And this element here represents my Perceptron. So if I go through and start the training process, then you can see it's very quickly learned to draw a line between the orange and the blue examples. So it's very quickly learned to separate the orange examples from the blue examples.

But if I make the dataset more complicated, so perhaps I add some more regions to it, then the Perceptron is going to fail to learn how to separate these two inputs. You can see it's really struggling to separate the orange from the blue. So let's try and develop something that will separate the orange from the blue in that example. The Perceptron only works if the inputs are linearly separable, and to get around this, we'll use connectionism and combine more than one Perceptron together. This is what we might start to call a neural network. Each one of these circles is itself a Perceptron. So each one of these has the linear combination of inputs and the non-linear decision making process. And I've got my input there, which is exactly the same as before. These are the Xs in the previous example. And then one or more hidden layers, and then some output layers, to sort of congregate the results and do the classification.

So let's just have a quick look at how this works. If I add some more neurons to my network. So I might build up something like this. And now I can train the network, and the multilayer Perceptron manages to successfully separate the two classes. If I make the data even more complicated, the network might take longer to train, or it might fail to do the classification at all. So with this very complicated spiral dataset, you can see that even this larger network is struggling with quite a long training time. It's still failing. So I might have to add even more neurons. Even more layers to my network. And I might eventually get to something that works. If I left this training for long enough. You can see it's very slowly working towards a reasonable result. Right.

So how to apply this. So computational musicology aims to answer questions like: Given a melody, can the computer generate a reasonable harmonization for that melody? Can the computer compose new melodies from scratch? And can we actually learn about music from the process of going through... Can we learn about music itself from artificial intelligence? So can we analyze chord structure in a piece that -- it would just be too tedious for a human to go through and annotate all these chords? Can we then compare genres of music using the metrics that we extracted from the piece? Let's have a look at how we might go about doing this.

We've actually got all the tools that we need already. With the multilayer perceptron, we can do a pretty good job. I'm going to address the problem of harmonization. Given a melody, note one, note two, note three, all the way through my piece, can I produce -- and the harmony up to the current point in the piece, can I produce the harmony for the current node, if you like? Can I produce a chord that would sound reasonable with a current chord in the melody? And yeah, I can train my neural net on an example of notes and chords and previous harmonies, and I can slide this neural net over the piece and gradually build up the harmony note by note, as I go through the piece. And this works okay, but it has a number of fundamental issues.

So one of them is: How big to make the window. How much context do I need? How far back in the piece do I need to go in order to produce the current chord? To go with the melody? And really, we want the network itself to learn this. And we don't want to have to fix the context. Because presumably at the beginning of the piece, we want quite a small context. We might not have any previous notes at all. And then as we go through the piece, we might want to learn more about -- we might want to use more data here. So there's also this trade-off between how much context we use from the current example, the current sequence, and knowledge gained from the entire training sequence. The sliding window approach also restricts us to one output per note in the input sequence. So this might be fine for harmony, but if we want to compose a melody, perhaps we don't want to have to output one note every beat exactly. Or we might want to produce multiple notes per beat. We might want to be able to produce a variable number of notes per beat. And sure, you can hack up the sliding window approach to sort of do these things. But it would be nice... It's not what we're doing as humans. So it would be nice to try and find a better solution. And the answer is: The recurrent neural net.

So this maintains some hidden state, as it goes through the piece. And it can... If we... So this is really the structure of the current neural net. We've still got an input and an output. And if you unfold this loop, then you get something that looks like this. The input at the current time, input at the previous time, input at the next time, and associated outputs. And it can maintain this state as the network goes through time. And you can train this network through time. And it's got this unit known as... Well, it's got many of these units, known as long short-term memory. Short-term memory refers to context from the current piece. And the long-term memory refers to context from the entire training set. And using this sort of more complicated series of operations, so instead of just having one activation function, we've got the non-linear operator in our perceptron. We've got a few of them, all working together to control how much knowledge comes from the previous notes in the sequence and how much knowledge comes from the training set.

And we can use this to compose music, harmonize music, all of the tasks -- solve pretty much all of the tasks we've talked about. So here's an example of a paper from a few years ago. Using a recurrent neural network to compose melody. And this network is trained on a corpus of classical piano music. It uses a symbolic representation of the music. It's trained on data, corresponding to notes. Not audio files. Not wav files. It's trained on MIDI files. And the result sounds something like this.

(busy somewhat discordant piano music)

 So we can see from that that it's picked up something about what humans find interesting in music. It's learned something about melody. It's learned something about what we find pleasing to listen to. But it did get stuck in a repeated chord structure at the end there. So it's getting stuck in the loops, and it hasn't learned enough to be able to get out of those loops. So another system is BachBot. This is trained on a corpus of Bach chorales. And we'll have a listen to this.

(piano chorale)

That's really picked up some of the key aspects of the corpus. The chord structure, the melody. And it's composed that original piece of music from scratch. And BachBot can also harmonize existing melodies, as well as doing the composition. A more recent example -- and there was a talk on Stage B just earlier on WaveNet, which is Google's... What drives Google Voice. It drives Google's voice generation system. And there have also been some successful examples of using Wave Net to compose music. But instead of using a symbolic MIDI representation, it uses audio files themselves, so it detects something about what we're actually hearing, rather than a symbolic representation of the music. One limitation in computational creativity in music is the quantity of training data available.

And this is really the thing holding composition up at the moment. In that in any one corpus, you have an order of magnitude too few chord progressions, really, to extract the essence of that corpus. But another area that doesn't have this problem is computer vision. And image processing. And here we have orders of magnitude more training data, and get a lot more successful results, as a result. We do need a slightly different structure of neural net. We could use the multilayer Perceptron exactly, but we would have a lot of neurons and we wouldn't exploit anything that we know about images. So we might want to be able to... We might be able to recognize a feature regardless of where it's positioned in an image. And so we put some of this knowledge into the structure of our neural net. So we're able to achieve this. So one system that uses this is Google's Deep Dream. And the idea is: They train a network that can recognize features in an image.

And then they sort of use the network in reverse. So they'll train the network to recognize features, and then put in a new input image, and then change the input image. So modify the image at the input. Until it gets some good response as an output class. So if you want the image to have more face-like features in it, for example, you would find the neuron that outputs face-like features and then modify the input until the neuron that recognizes faces has a high response. And this generates the sort of trippy, psychedelic images. Here it's got a wavy texture. This is my input image. This is the output after 20 iterations of this process of Deep Dreaming. So it's done something artistic to the image. Here it is where the original dreaming network was trained on a different set of images.

And it's output some higher level features into the image. Here's another image. I'm not sure if you can see this. But it's recognized a branch-like structure, and then sort of replaced the branch with some snake-like feature in the output. I don't know if that shows up in the projector or not. But it's doing much more than superimposing images. It's really learning something about the structure and modifying the output in an interesting way, in order to respond to that structure. And some people have left Deep Dream running for much longer on the input images than I have, and come up with these really artistically impressive images. The final thing I want to briefly touch on is style transfer, which is another very general approach, which could be applied to any area. Music, images, literature. That takes one example of the content image -- the poem, whatever -- and sort of a style that you want to put into that image.

And in quite a novel and general way it combines these features. And you can try this yourself. If you grab that URL from the recording or whatever. But I can take the -- the code is all in Python and accessible. I can take my input image of a squirrel and some style that I want to try and redraw this input image in the style of. And I get this quite neat output. Here it is with a different style. Here's evil squirrel getting back at me for using him as an example. And yeah. These results -- quite artistically pleasing. Quite effective. I can do the same thing with a different content image but the same style images. And again, I get quite an artistically pleasing image each time. People have also done this for text, to invert sentiment. So turn I would recommend find another place into: I would recommend this place again, going from a negative to positive sentiment. And in the reverse direction, going from positive to negative. Really good food that is fast and healthy to really bland and bad and terrible. So again, it's not perfect English. But it's picked up the... It's identified the key features here.

So in conclusion, there's still a long way to go. But we've come up with some things that we find pleasing to look at or listen to. Ultimately, is the computer being creative? Or is the neural network the 21st century paintbrush? Is this another tool for humans to express their creativity? Is the human who designs the algorithm creative, or is the computer being creative itself? And perhaps that's an open question to discuss. But at least we've got some insights into possible algorithms and techniques to carry on from here. So thank you very much for listening. And we should finish there, to let them set up the video. But I'll be around if anyone has any questions. Or feel free to drop me an email. Thank you very much for listening.

(applause)

>> So that was computational creativity with Matthew Ireland. And he'll be stepping outside for questions if you have any. And whether or not you have any questions, something that you can do that will help make EMF even better...

Live captioning by @stenoknight for White Coat Captioning @whitecoatcapxg, whitecoatcaptioning.com, and sponsored by Nexmo
