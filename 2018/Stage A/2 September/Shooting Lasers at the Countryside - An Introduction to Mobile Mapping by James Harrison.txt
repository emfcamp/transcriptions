Shooting Lasers at the Countryside - An Introduction to Mobile Mapping by James Harrison
>> Our second talk is shooting lasers at the countryside for faster internet. . 
>> I'm James Harrison. The talk on mobile mapping, which is an exciting and reasonably new field, which is starting to become accessible - almost. So I work for a cup called Gigaclear, previously at BBC R&D. I occasionally make things, and - Robot Wars, ends up getting cancelled. Obviously, I'm not an expert on any of the things that I've talking about but I've broken enough stuff that I feel I should come and share some stories. A bit of a Jacques ground on the company we work for. We want to give people faster internet in the country. We build fibre paths, new infrastructure to homes in rural areas of the country, and that means we've got to get fibre from a distribution point to properties over a very large amount of ground. We've built around 0,000 properties so far, with doing another 300,000 or so, we're hiring like mad. Please come and talk to me. We're in Abingdon in Oxfordshire. We drive around the country with small ploughs, and that sort of thing. We are interested where we can do a soft dig. Doing this is really cheap and fast. If you need to start digging up concrete and things, that's expensive, and we try and avoid that. There's a straightforward algorithm for this. We work out where people can want fast internet, we work out what it costs, and then we build it. The first bit is easy, modelling where people want fast internet is easy, no data sets. Building it is also really hard because it turns out digging up 10,000 kilometres of countryside is actually quite difficult - who would have thought! But the bit in the middle where we figure out how much it will cost and build those areas at all, that's interesting. Because, if you get it wrong, that's a bill deal. If you get it right, that lets you build areas that people didn't think were viable if you model that well, that's great. The trouble with this is that existing rural mapping sucks. This isn't a digger OS. Mastermap is the commercially available best map of the UK. Bits are in open data now, so, if you're interested in this, look at the survey's website. They've got a great map, the whole country, and it's a cadastral map, a complete set of polygons that cover the surface of the UK. It covers a lot of thing like verge, very thick hedge, which is really quite tricky. We have had it miss things like streams and drainage ditches, and sometimes we can get dry-stone walls, the OS haven't resurveyed the area in years, and you try to dig holes in it, and you realise there is a wall there - and we have had houses and things. This is the sort of thing we sometimes found. There's a hedge there that is quite thick. Again, is where it was missed out there's a house built on the side of the road here. In the maps, this looks like a continuous piece of verge that we can plough through. Not going to happen, unfortunately. This is significant because they eventually cost us a bit of money, and that affects what you can build in the area. So we send people out to go and survey an area, and it turns out that takes a lot of time and people. If you're building fibre into the home in urban areas like BT and City Fibre, and other people, you're averaging 10 metres between properties. The average is 80 metres. Straight out the gate, there's a lot more stuff to do. Trying to get all that data back, you can send people out to go and look at things and try to understand the terrain whether there are any obstacles or so on, and getting the information back in a way is useful that lets us then correct those problems is really hard, and trying to manage that much information at scale turns out it's - you don't want to be doing it if you can help it. Satellites are not useful for us, because they can't see through trees, and lots of our routes are covered in trees. And they don't have a great resolution. What is going on with the display? So, while we're trying to get this sorted, aerial photography is another thing we looked at doing. It again has the same issues with tree cover. You can get oblique imagery where someone has taken a picture of the side and looking at things edge on. That can help. In most areas, rural areas, it doesn't help that much at all. Google street view is another option everyone says just use street view. Most of the countryside Google didn't resurvey since the original survey, so six or seven years old is normal in rural areas which isn't very useful for us. Mobile mapping is where we come to. Basically, it's a pro access for capturing geospatial data from a mobile vehicle with a bunch of sensors strapped to it, normally including things like camera and sometimes things like LiDAR. In order to do that, you need sensors. You need to figure out where they all are. You need to put that in a vehicle, get that information somewhere usable, and somewhere accessible to make some tools work for that data. So the main census we're talking about - the main sensors we're talking about today are LiDAR, which is a form of laser scanning, and cameras. Cameras aren't useful for photo mapping. We have a classic case where we take lots of photos, we think it is verge, it's a bank at 45 degrees which is a real pain for us because we can't plough in there effectively, so you try to figure that out from photogrammetry. Cameras are really good for context. Laser scanners are two main times - pulse-based and phase-based. Pulse-based one are keeper, and you get a good facial recognition. It's a pretty simple idea. You fire a pulse of light. You wait and see for any reflections that come back at that particular wavelength you've shot the pulse at, and you then have a think about it, figure out where those things that you just saw were in space based on time, so the nice thing is you can do detection of multiple objects with good enough scanners. If you fire a pulse of light at the tree, the light might through a leaf and hit a branch behind it. You will see a reflection from the leaf, and a bigger reflection from the branch which stops the light. This works really well for verges and things because it means you can shoot through things like Hedges and catch enough of the surface behind the hedge that you can actually measure what the verge looks like below vegetation and you can if I can out what the vegetation looks like, how much there is, and so on. The scanner is from a company called ... pulse-based, can fire one million point a second. It has a mirror in the back of the scanner which can spin at 250 revolutions per second, so, 2,000RPM. That spins around and we effectively drive along with this on the back, and it spins around. Each time it spins, it's firing out a large number of pulses we capture information from. This gets you initially some events that are in the scanner's own co-ordinate system, so basically the automobile it saw it at, and an X and Y off set. Once you georeference that, you get a point cloud. Point clouds are literally a cloud of single point in space, and that - each of those points will have attributes with it, whether or not you saw multiple echoes, whether the point itself was a first echo or last echo. We can have it trim information in there. We don't get colour because we are shooting in infrared. We basically get an intensity view of the world in infrared but we can then overlay with images to make some coloured.  In this year criticise, it might or might not work. Yes, kind of. This is actually a point cloud of the campsite which it controls, so this will be interesting. And so this is actually a complete scan of Eastnor campsite and grounds. This has been georeferenced and coloured so we have a lot of metadata information about it. This is the output we get. This is online by the way if you look me up on Twitter, you can find the link to this. You can download the whole data set for tree. Thank you to my company for letting me drive my vehicle around here and collect all this. This data set is the output of about hovel an hour of driving around the site, and then about two weeks of fiddling around processing it. It's not usually that long. I don't usually do this stuff hands-on. Normally, it takes us anywhere between three to five days of processing to get a decent output. The nice thing about a point cloud is that it's accurate within itself as long as your georeferencing is reasonably good, so, within a particular scan line, you might be talking about two or three millimetres of accuracy which when you're driving along and doing a million of those measurements per second, it's pretty impressive. You can take angular measurements, distance measurements using whatever tools you like and figure out a lot about the world without having to do a huge amount of work. All of these things are pretty safe. If you start doing aerial LiDAR, that changes. Most aerial scanners will be class 4 and will burn your eyes out. All of the stuff we do is very safe, nanosecond pulse lengths. Other stuff you can get: if you're looking for a hack to look on - have a range of cheaper sensors. They're still around £2,000 to £3,000. That's cheap in LiDAR terms. That's as about as cheap as it gets today. Plus F are probably the gold standard. They're phase-based sensors. Those are very, very expensive - six figures and up quite happily. We also want to take some pictures. If you're driving along at speed and want to take good pictures, you need to consider a couple of things. You want a low dynamic range in your images, 10-bit or 12-bit dynamic range. You drive along and imagine you've got trees on one side of you and bright sunshine on the other, you can still see detail in all the trees down here, but you can still see the detail on the bright verge to your right. We do some post-processing at 8-bit so we can consume it easily. Because we're driving along and mobile mapping rigs, we want to drive along at carriageway speeds, we want to do up to 70 miles an hour. We don't want any tearing in the image, so we want a sensor rather than being a rolling shutter where the image is read from top to come, there's a global shutter so we can take the whole image at once. There are - plainar cameras get used on mobile mapping rigs to add extra information to panoramic imagery. You might be looking at the - panoramic cameras are what you need if you want good contextual images that let you kind of get that Google streetview feel which is a big thing from a usability perspective. This is hard to use unless you think through how you're going access it. Most mobile rigs use a camera from Fair called Ladybug 5, 30 mega pixels. If you're ever doing anything with rugged connectors, never put micro-USB3 on it. A - we've killed three so far, I think. You don't want those. We now to figure out, we've got our sensors, we need to figure out where they are at that moment in time top we need to use a co-ordinate reference system. In the UK, we have the Ordnance Survey National Grid, and that's got a good relation to the geo referencing used by satellite systems. It doesn't move over, whereas if you try to reference things to latitude and longitude, due to continental drift, your measurements will be wrong over time, so about 2.5 centimetres at the moment. GNSS, GPS is one GNSSS system, Galileo, and other GNSS systems, those are how we figure out where we are crawl. Used to be able to to do triangulation and things, and nowadays it's pretty much the surveying for a GNSS survey done right. You can get pretty good accuracy without needing any external correction sources but - if you want to get better than that, you've got to correct for a bunch of different areas, mostly at months faring areas, and the only way to to that is use a better antenna to get better signals and reduce the effect of things like multi-path interference, use more satellites so you've got more data to work with, and work out what the satellites were doing at your position, so having accurate information about where the satellites were. The only real thing you can do is use a base station to correct errors, so something that is stationary, so anything you measure in terms of movement, you know is error. We are kind of care about accuracy a lot in this, because when we get to processing, trying to make overlaps and things like that all make sense, if we have a more accurate trajectory to start with, then we have a much better chance of making it up at the end. So we're trying to aim for centimetres, not metres, and that's not an easy feat. You typically end up need ing put the base station down president Ordnance Survey have a huge number of existing base stations running all the time that you can get the data for. There are commercial services that will take that and interpolate a base station where you're doing your survey work which lets you effectively figure out what the error is at that particular moment being introduced by the atmosphere, and so on. That means you can correct your position down from metres to centimetres. Now, this is all getting a business expensive. It's getting cheaper, happily. Much as LiDAR will be soon. I will talk more about that later. But survey still on the £10 to £15,000. If you're looking for systems that are not survey-grade, but corrected with RT great, then you're looking at £3. 
700. It's starting to come down in price. A lot of the people are doing with drones has helped that a lot because suddenly you need cheap positioning because you might crash the drone. You don't want £10,000 a scanner on it. You can answer get open tools. You can get the correction data from OS for free. Once you've got a rough idea where you are in the world in absolute terms, we start to figure out where we are in terms of our attitude of the vehicle, so, as we drive around, we are rolling, pitching, we're turning around, and GNSS with multiple atennias receivers won't give us the - temporal accuracy and spatial accuracy. In order to figure out where we are at any particular moment in time, bearing in mind we are doing 250 points a second, we care about it being precisely timed, we need an inertial system. Usually integrated with the GMSS receiver so there is a box that does your positioning. So, typically, these are made with accelerometers, and those are exactly the same sort you find in your phone. Just slightly more precise versions of it which justifies the higher price-tag. And the same goes for the gyroscopes. More advanced survey systems will use fibre optic gyroscopes. Those are much more expensive. They're complicated. They are really expensive to make. They're really, really, really expensive. You're still looking at - you can still get pretty good performance out of cheap GNS receivers. It's good enough for doing a lot of things. If you're not doing laser data, you can use a lot of this stuff that is out there. If you're doing laser data - so these things will be accurate enough that you know where you are absolutely in the world without GNSS for about a minute, you might - accuracy still.  We also use GNSS as a base references. So, as - so, all of our sensor data is being captured and time-stamped. All our position data is being captured and time-stamped. We know at any particular moment where we were and captured. Things like event triggers for cameras are on that column time base. So we just need to get our GNSS together, get rid of anything that we had bad GNSS signal, or stuff we might introduce error with. We feed all of that data and inertial data, which is a way of the - so things like gyro bias, and things that let us figure out a better solution, and we run that out a few times, and we get accurate position data out. We are typically to help that process would be we actually drive half an hour before we start capturing any data at all, preferably on nice fast roads with slow corners in, and that gets us a good view of what the INS is up to before we start capturing data. What we end up from the GNSS is something that looks like this. This is green through to blue the estimate of the positioning accuracy we're getting from GPS. This is the east east site. We had good GNSS. We kept running the recording, and you can see we had drop-outs, periods where it was particularly poor, and those areas we typically would either exclude or just treat less trustfully in the algorithms that are processing the information. One other thing there you can see a green triangle which is the virtual reference session. Typically website once we've done all of this stuff with this fancy hardware, you can get down to a positioning error in absolute terms that looks pretty good, and we're saying there usually for X, Y positioning, down to three centimetres accuracy, got that at 480 Hertz temporal accuracy. That gives us a lot to work with. We're trying to get the scan overlaid on top of our existing maps because we want to use them for existing maps, so we're using targeting sub decimetre accuracy, so I can tell you whether or not you're in one of those tiles on the floor, or the next one over, or better. Trajectory processing: so figuring out that trajectory making it as accurate as possible. That's where we spent a lot of time manually trying to get it as good as possible because it's going to cause us loads of issues later on in processing. We're really keen to get that as good as we can. So once we've done that, we have some overlapping flight lines. This is again Eastnor data. You can see in different colours there, different bits of the trajectory that we are processing - trajectory that we are processing separately. Those are overlaps because we've driven the route more than once. You can see from this diagram over here we've actually got multiple posts appearing. There's only one post. We're driving past that to to the row of posts up there. Because we've got multiple overlapping scans with slightly different positioning, we're recording that more than once effectively. If we don't correct that, we end up with interesting side effects like this. This is where we've got multiple objects appearing, and, then again, you can see there's a lot of error here. This is something due to the fact that I've processed the data incorrectly. If you have angular errors, then you get different sorts of weirdness appearing. So the trouble with all of this is that everything, literally everything in there is proprietary standards, proprietary formats, and we have to get somewhere useful from the proprietary formats into open formats. We process our information and do basic classification of those points significant anything directly below where the scanner was was probably ground and work our way up from that to say this is ground, this is vegetation, this is maybe a building, looking at things like reflecting. We get rid of things like noise, so we sometimes, while we're driving along might scan rain - we try not to - birds are occasionally, you drive through a scan, and you see this weird bird shape in the middle of space. It's just happen to be driving past a bird flying past us. We go through the data set and try to remove all of those points. We then try and align all of those overlapping passes, and get everything snapped to a single surface and corrected for an X, Y error. It doesn't always work perfectly. Then we colour the points in by taking our images, overlaying on top of those point clouds or images and positions and casting rays out from the images. The images are much higher resolution than the point map. We've got a lot of pixels to work with and take an average of three pictures. So, unfortunately, the hardware is inaccessible for this to have become an open source thing. This is going to become more accessible because the cost of the sensors is going down, and there is a great deal of data, and lots information you can find there. I've got three minutes left, okay. So there is lots of potential for new algorithms we haven't worked with yet. We generate loads of data, about a terabyte a day. We store all of that and use AWS for all the processing because it is cheap and easy and scales pretty well for this. We're capturing 50,000 points and two billion points of data per day of information about the world. We will get rid of some of the photos and things because we don't need all of them. We will keep the point cloud for life. We use Kujis a lot which is our open source system. We built plugins to access the information from all of that. We didn't build our own one, bought country from 3D laser mapping. There are lots of other companies that do them. We end up with a van like this. That's bright orange. In the back of the van, we have our system that looks like this. We can see a laser scanner there, the camera on top, and the GNSS scanner on the top of that. We start processing and off we go. Other systems out there, things like this is the Pegasus 2, got two of our scanners. You can record more points. So this is the kind of stuff you get out of it. This is from kind of over there. And this is the intensity gradient so this is showing - gradient, showing the differences in reflectiveness between different points. You can see where we are driving over, it is actually some tracks, so those will reflect more brightly in infrared, so we will get a higher return value. With things like trees been branches will reflect more brightly than the vegetation will. You can take good structural images of trees. Things like removing vegetation to literally strip a tree back which is interesting. So the RGB stuff mostly works. You can see at the top here white pixels where we couldn't find any colours. There is a wire-link fence down there, and you can pick up these individual wires. No problem at all with a very fast scanner, you can pick up ... or pictures of things not being properly aligned. So you can do some interesting things with this. Like you can take height maps and use those for figuring out where you should put things in the world. There are lots of things that you can do with this data. It is really, really powerful. There are all sorts of errors that you can interviews by processing it inaccurately. Here's a slide issue where these are not obviously natural features, and we've got a couple of flight lines down here that are perfectly fine but one that isn't. [Off mic] ... because we've got it. Some things we've hit on the road. You should always check to make sure that the roads are not private roads. We were warned off this particular road by gentleman with a shotgun. We did not hang around. And you should always check the height of bridges! Before driving under them! Very quickly, because I'm being hurried off, if you want to do this yourself, you can do a budget version. Panoramic cameras are getting cheap, laser scanners are getting cheaper. Obviously, self-driving cars is starting to push real change towards solid-state sensors which will be drastically cheaper than the things we get now. If you don't need perfect precision or accuracy, you can get a long way with what you can get off the shelf for not a loft money today. If you want to build these things, you're looking at £500,000 plus. If you want to do is on the cheap you can do it more from £2,000 to £5,000. In a year or three's time, it's about £500. It's awesome to take a look at people at speed. You can actually put that data in the hands of more people. If you want to play around with the Point Cloud data, a link in a second. Great tools are available. If you've got the three tools, you can do a huge amount. And, yes, there's a link there for the data if you're interested. Come and ask me questions, and, huge shout-out to Andrew Godwin who made a 3D print of the campsite which is you can look at the top here. [Applause]. 
>> Thank you or your talk. We are still looking for people to volunteer on stage. The EMF website has a page for feedback. If you want to leave some feedback about your time at EMF, the good and the bad. 