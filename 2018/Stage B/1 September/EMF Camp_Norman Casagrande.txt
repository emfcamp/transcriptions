      Up next:  WaveNet, what's behind Googles voice?
      
      Norman Casagrande
      
      >> Hello?  This is stage B.  Welcome to stage B.  Before I introduce the speaker, some announcements.  First of all, tomorrow 10:30 a.m., there are five satellites flying over the field.  If you go out at 10:30 a.m., we would like to make a sculpture which will be visible from space.  It's EMF wolf from space.  It will be a sculpture of a howling wolf.  At 10:30, you can go back to the tents.  But we need as many people as possible to turn up at 10:30 to be seen from space.  That will be cool.  Secondly, we need mobile and chairs.  We need audio and visual recording for the talks tomorrow.  If you have a half hour to help, go to the green room to sign up.
      Thirdly, at the Cy Bar tonight, there's a tech swap.  You can bring any old gadgets that you no longer want and walk with someone else's old gadgets.  Sounds like fun.
      And finally, there will also be a crappy robot competition at 7 p.m.  Which is in 10 minutes.  At the bar.  Okay.  Those are all the announcements.  I would like to introduce Norman Casagrande from DeepMind who will talk about WaveNet.  What is behind Google's voice?
      NORMAN: Welcome to my presentation.  I would like to start with a teaser.  This is a recording.  I will play it first.
      >> This is boring, Harry moaned in front of the tank and looked intently at the snake.
      NORMAN: I'll play it again just because it didn't get the first part.
      >> This is boring, Harry moved in front of the tank and looked intently at the snake.
      NORMAN: This is a voice which has been entirely generated by computer.  There was no recording of a person saying that sentence.  There was no even part of that recording that were stitched together, you know, to generate that.  This is really entirely generated voice by a technique called dip loading.  But I have a look at the technology and what drives it.  But first, a little bit of a history of speech synthesis.  A has a long history.  We can go back to the middle ages where people built this brazen heads which had  were making some sort of sound that reminded of human speech.
      In this picture it was showing how it could be used to actually scare people.  But the desire for generating machines that can then mimic the voice of human beings actually goes a long way, way back.  In the 20th century we started using a little bit more advanced technology.  This is a schema for what it was called.  Sort of like pianolike instrument which could be used that could generate some sort of like spoken audio.  But it was mildly intelligible.  But it wasn't really that much useful at the time except for entertaining people and making this machine was spookingly generating audio.  More recently we have voices which became very famous like the one that Hawking used back in  well, until his death.
      Now, more from a software perspective, traditionally techniques from a software point of view have been using two different approaches.  Mainly at least.  One of the unit selection.  Basically, you have someone in a studio reading a long list of sentences.  It can be very boring.  We're talking about tens of hours all the way up to 200 hours.  And you take a software.  You slice those recordings into bits and you try to stitch them together.  It does sound natural.  Especially for the parts that were part of the original recording.  But if you're trying to say something that it's not part of the original database, it might sound a little bit cliche.  I have an example.
      >> The avocado is a pearshaped fruit, smooth, edible flesh and a large stone.
      NORMAN: You can hear in the parts where it's trying to say something like a word, it sounds good.  But in between words or sometimes in between, you know, parts of a word that was not part of the database, it sounds really, really bad.  An alternative, which was also quite popular, it was called parametrics.  So, with the idea was that you would try and simulate the vocal track of human beings with a formula, essentially.  And then you would use a set of parameters where you manipulate this formula and you generate audio.
      And you can also try to adopt those parameters based on the speech that you have the quality on.  You should have recordings.  Now, this work when is you want to generate pretty much anything.  It sounds quite unnatural.  You have another example here.
      >> The avocado is a peershaped out from, smooth, leathery flesh and a large stone.
      NORMAN: It might not sound too bad, but with headphones, it will sound really, really bad.  And in particular, if you don't have a lot of recordings for the system, it doesn't sound good at all.  And only recently deep learning has become prominently as a technique to actually take the best of  the best of the world.  We take  we learn a system endtoend to manipulate or generate the audio from the samples themselves.
      And this is an example of the same sentence generated with WaveNet.  And we're going to look at what WaveNet is.
      >> The avocado is a peershaped fruit with leathery skin.
      NORMAN: Okay.  I just wanted to make another, you know, listen so that you can feel the difference.
      >> The avocado is a peershaped fruit with leathery skin, smooth, edible flesh and a large stone.
      NORMAN: The next one.
      >> The avocado is a peershaped fruit with a 
      NORMAN: So, it's way more natural.  Now, what is WaveNet?  Well, the definition, the actual definition of WaveNet, it's an aggressive neural network with dilated convolution which can be summarized into magic.  Which is basically what deep learning is most of the time.  I tried to, you know, keep the memes to a minimum.
      But anyway, this is EMF.  And so, we want to dive in, you know.  Open the hood and look exactly at what this in your neck of the woods and it means.  So, first of all, the neural network part.  Now, I'm not going too much into details about what the neural network is because I only have half an hour and there's lots of really good tutorials out in the web.
      But in a nutshell, you have a bunch of  oop.  Let me take a pointer here.  You have a bunch of people.  You make them go through the hidden layers.  You basically take the input and multiply with a bunch of matrixes that are the parameters that you need to adjust.  Apply linearities, pass to the next level, blah, blah, blah, and all the way up to the top.  And in this top is what you're trying to predict.  And basically, depending on how well your prediction is, your wait, we're assuming the parts here  to make sure that the next time you run with the same example, you're actually closer to the output there.
      Now, what were the outputs for WaveNet?  Those outputs are the samples.  This is part what made WaveNet so revolutionary.  We were trying to model the samples themselves.  And the samples are those values, okay?  That exist in the  that form the representation on the audio, this waveform.  I have this animation, for a good quality audio, about 24,000 of them per second.  So, you need a model that it's able to predict 24,000 of those tiny little samples per second in a way that make it sound natural respect to the order.
      So, how does it work?  Neural network is saying, you know, the neural paths, well, next bit, it's the ultraaggressive part.  Because we are actually proceeding one sample at a time.  We start with the one at the very beginning here at the bottom.  Which, you know, when you actually  when you're training you have the ground truth, usually.  Actually.  So, the original audio.  When you're not training, you might want to start with random or zeros depending on your thing.  Anyways.  You make it go through the  these hidden layers, notification, blah, blah, blah and so forth.  And then you get the output.
      Now, as you remember, you said the neural networks, you have the backstage where you adjust the waits here.  Long story short, you get a number at the end which is your sample times that.  Well, then you pick up that sample that I had, and you use it as the next input.  And then again you go through the network which, by the way is the same, okay?  Just to be clear.  In this case those who samples are different.  But the network in the middle is the same.  Right?  So, you readjust those things a little bit in support.  So, you repeat, you repeat, you repeat.  And then you end up having a lot of those samples which are generated.
      Now, last part  oh, go in and obviously you've got a lot of them.  Now, the problem is that you have a lot of them.  Previously I was showing 24,000 her second.  And generally, when you want so say something, it's more than a second.  It's 5 seconds or something like that.  So, an additional critical contribution by WaveNet is this concept of a dilated convolution.  So, instead of just taking  picking up the output of, you know, your input, stick it into the hidden layer and it goes all the way up in the linear fashion.  You pick up the one that comes before, at least with the first layer.  And then for the layer above you have like this concept of dilation which means that it picks up the output of the previous layer which is in a way that it's a factor of the time stamp.  So, in the case of here, it's a factor of 2, and here is a factor of 4 and a factor of 8 and so forth.
      The hidden layers are representing a summary of the state of the knowledge of that point.  These samples  the one that you're generating at end, it's actually encapsulating the knowledge that goes not just from the step before, but goes all the way back to, you know, a power of 2 depending on the number of lay theirs you have in between.
      Okay.  And in summary, you have like this fancy animation which was generated that was created when the blog post about WaveNet came out and shows the whole process.  And you see here, you start with the input, the output, and again, and again and again.  Until now we have been talking about samples, right?
      And in particular I hinted at the fact that when you don't train, and you actually want to generate audio, you might start at the very beginning with silence or a random number.  But if you trained on a lot of audio  of spoken audio  what will happen is that the network will learn to say something, okay?  But just anything.  Okay?  Nothing in particular.  Something that kind of sounds like voice but doesn't mean anything.  And I have a bunch of examples here that are quite  set.
      [speaking]
      So, this is a WaveNet that has been trained on tons and tons of spoken audio.  All right?  Then you have a random number at the very beginning and you run it through and just tell it to generate sample after sample after sample and you get something that sounds like Swedish.
      >> [speaking]
      NORMAN: There's another example.  It's gibberish.  But it's gibberish, but it's very much believable with respect to what the human being would say or, you know, the way the human being would pronounce stuff.  So, how do we tell WaveNet what to say?  Well?  Oop.  Not this.  I don't want you to say that.  All right.  Well, there is this other part which we call the conditioning stack.  Okay?
      In this conditioning stack well, you know, you have at very beginning what we call these linguistic features and we stick those as the inputs, okay?  To the network.  And at the beginning, so the linguistic features are usually things like phonemes, here you have to put a stress, punctuation, question marks, points.  So, the network learns that it has to break for a moment and so forth.  And importantly every language, obviously, has its own.  For that matter, every voice has its own.
      I mean, I would challenge anybody to try to replicate what it's doing.  It has its own unique way of saying things, right?  So, you need a set of those features that you want to match for a specific feature, for a specific language.  Now, these at the beginning  so, you have an input.  Anybody knows anything about neural network will understand that this is not as easy as the previous example that we saw because for a sample you have a number.  In this case we have phonemes.  So, you have to map those phonemes in a sort of way.  But long story short, but there's also a deep learning technique.  If you want, we can discuss those things later on.
      But anyway, they got mapped into neural network stuff.  And then they squeeze and then rearrange, et cetera, et cetera, so that they can match into these  into the existing hidden layers.  They're literally added to the hidden layers.  So, you can imagine that at this stage here, okay?  This mishmash of linguistic features end up being a vector of numbers that gets sucked to the vector that's represented by those waits.  They're represented as wait.  And this is the web of magic.  And it's really, really cool.
      It actually made a huge splash, and nobody was really waiting for something like that.  Now the problem  oh, yes.  And the results speak for themselves.  I think we already had a bunch of examples.  Make I'll play just one more to give you, you know, a sense.
      >> A single WaveNet can capture the characteristics of many different speakers with equal fidelity.
      NORMAN: This is unit selection, stitching things together.
      >> A single WaveNet can capture the characteristics of many different speakers with equal fidelity.
      NORMAN: All right, and the version.
      >> A single WaveNet can capture the characteristics of many different speakers with equal fidelity.
      NORMAN: All right.  Now, this was all well and good in 2016.  However, it was unfortunately a little bit slow.  Something that I didn't mention was this process of going back and forth, back and forth, back and forth through a lot of those hidden layers which were holding  I mean, I barely skimmed on the fact that you have metrics multiplication.  But there were a lot of those metrics multiplication.  So, it was great, but it was really, really, really slow.  However, people recognized it was an extremely exciting piece of technology.  So, there was a huge push for making it realtime so that we could use it to power Google Assistant and Google Home.  So, in 2017 we came out with a different piece of technology which was sitting on top of WaveNet.  And just to give you an example, the original WaveNet  so this is showing you how much time it was taking to generate 0.02 seconds of audio.  About a second.  So, bear that in mind for a moment.  Right.
      Oh, and by the way, like they say it was a really good technology, and we launched it a couple months later.  Anyway, how does it work?  Unfortunately, I don't have a lot of time to go into details because we're going to also talk about version 3.  But in a nutshell  remember, this is the autoaggressive part.  Basically, there is a technique in machine learning that is called distillation.  It's magical to a certain extent because it means that you first train this model on a bunch of data, okay?  Then you have this model which is okay, it's very good, but really, really slow.  What you can do is you can pick up like a model that trained on the output of the first model.  In a way it's kind of like, you know, a computer talking to each other and one email.  You should do this, you should do that.  Without any oversight from human beings or, you know, additional data, right?
      And this is what the model is about.  Funny thing is that the actual feedforward model starts with noise, okay?  Goes through a bunch of dilutional layers and then it outputs the result in one go.  Instead of going through this back and forth model.  Yes.  Unfortunately, I don't have a lot of time to discuss this.  It's quite interesting.  There is a bunch of papers.  But anyway.  More interesting, I think, well, first where it used to take 1 second to generate the 0.02 seconds of data, now this thing can produce 20 seconds of data in just 1 second.  So, it's really, really, really, fast.  And this was 2017.  And this is actually what currently is in production within Google.
      The only problem with it is that it takes about  it's a bit complicated because it takes some time to train the first model, the one they teach.  And you have to train it a lot of data on the speaker.  And then you have to take the other model.  And then this student model has to train on the teacher model.  It's kind of a complicated process, right?
      So, in 2018 we further advanced research.  So, you see slow, complicated and make it even faster.  And so, remember, this was the original schema of WaveNet, right?  Well, this part is gone.  Right?  And it has been replaced by, well, sort of this bit.  This bit.  Which is a cell.  Now, bear with me for a second.  We're not going into too much detail about this thing.  Just so you know, this is a quite common technique in machine learning for reoccurring processes where you have sequences.
      But just give you an idea, the input goes here, goes through a bunch of stuff.  Mathematical operations.  It's basically deciding what to remember, whatnot to remember, et cetera.  And then it goes out there.  And there is also a bunch of stuff that goes into the connection between those cells, so they can talk to each other.  But the beauty about this new system is that it uses way less parameter in this area.  It can talk to each other like in between the timestamps.  It doesn't need dilation.  Not in this bit, at least.  And it  oh, yeah, and by the way, the conditioning goes somewhere here.  This is very scientific notation, by the way.  Yep.
      And it's a very compact model.  And about that, it can also be sparsified, I won't go into too much details because this is something that I could talk for hours about that.  But in a nutshell the idea is that these parameters that the model is adjusting to a training process.  It's just a bunch of values, right?  They're called weights, all right?  And not all of them are really that important for the actual process of generating, you know, the output that we care about.
      And if you, during training you could decide to say, okay, look.  The reduce waits that are not that high, that are not really contributing too much to the results, just zero them out.  Right?  And then grow on for a while training.  And then you get another bunch of waits that are also kind of tiny and you say, I don't care too much about those.  And the model  because this is a training process, the model has to cope with that kind of situation.
      And eventually you end up having a model that is extremely sparsified.  Most of the waits are zeros.  And after you train the model, you can ignore them, they don't contribute to the process.  To the point that you can this thing on a device.  A device like a phone.
      Now, what about the future?  Well, we want to make it even faster, obviously.  There's a bunch of ideas.  I can't go too much into details.  But I think that even more important than that, is make it train faster.  It is true that this second model  third model that I was talking about doesn't need the teacher/student approach, so you can train it in one go and deploy the thing as it is, but it still needs a lot of data.  However, there is different techniques that we are looking into to, you know, just learn based  instead of using tens of hours or hundreds of hours of recording, we can maybe train it in 10 minutes or something like that.
      And bring it to the whole world.  Because right now since we kind of like have data constraint, the Assistant is only available in English, German, French, Japanese and a bunch of other languages.  But we would really like to bring it to the entire world and to all different types of languages.  We also have a bunch of additional ideas for that and I'm pretty sure it will be really cool.  But I think more important than that is you could use this technology to other things outside of texttospeech.  Because WaveNet can replicate signals really well.  And I have an example here.  This is WaveNet trained on music rather than sound.  Than voice.
      And this is  whoops.  And this is what it generates.
       [piano music playing]
      NORMAN: Now, this is like the babbling that we were hearing before.  Right?  There's no conditioning.  We're not telling WaveNet, I don't know, play a specific set of notes.  We're telling WaveNet, train on music.  Now figure something out.  Start with random.  And this is what it generates.  And there's another one.  An example.
       [Piano music playing]
      I think it's really cool.  I don't know if you're  anyway.  That's the end of my presentation.  Whoops.  If you have any questions, do we have time for questions?  Ah!
      [ Applause ]
      Thanks.

