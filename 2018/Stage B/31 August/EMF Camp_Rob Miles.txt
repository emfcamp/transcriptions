      AI Safety
      
      Rob Miles
      
      >> Good afternoon, stage B.  My name is  my main purpose at the moment is to introduce you to the absolutely brilliant Rob Miles talking about AI safety.
      [ Applause ]
      ROB: Thanks.  Hello, everyone.  I want to clear up a couple of things first off.  Or, you know, start off with some things.  I want to first off say that I am not an actual expert on this subject.  I have a YouTube channel where I share this research because I think it's important and I think it's really interesting.  There aren't have many experts in the world.  I'm not one of them.
      I'm also not Rob Miles the lecturer from Hull University.  He's here somewhere.  If you're looking for him, he's around.  Anyway.  So, and, we don't have quite as much time as I was hope, so, we're going to move fast.  This is the question that I'm trying to get people to ask themselves with this talk.  What is the most important problem in your field?  Take a moment to think about it.
      And why are you not working on that if you're not working on that?  I'm going to argue that I think the most important problem in the field of AI and possibly in the field of computer science and possibly in the world is AI safety.  So, why?  What's the risk here from AI?  Pretty much you can divide it up into four categories along two axis, right?  You have your shortterm stuff, your narrow stuff.  This is the kind of problem that you can get with AI systems as they exist or in the very near future and this varies by  by misuse or accident, right?
      Is this somebody using this technology with the wrong aims, or is this somebody just making a mistake and the technology is doing something that's unintended?  So, I have some examples there.  I don't have time to go through them.  What I'm mostly interested in, and what this talk is mostly going to be about is the longterm risks or the general AI risks.  And I feel like the general AI systems are so difficult to understand and control that it almost doesn't matter who is using them.  So, the risks from misuse are quite similar to the risks from accident because the chances are  it's unlikely that somebody's going to make general AI and use it to do something bad.  It's more likely it's going to do something bad entirely of its own accord.  So, that's the most important problem.  Thank you.  Wow.
      >> That's what I could get.
      ROB: That's a lot of water.  Okay.  So, this is like the statement of the problem.  I think that we will build an artificial agent with general intelligence.  And I'm going to explain what I mean by them.  Sooner or later, what do I mean by that?  This is a survey from a bunch of AI experts.  These are people who published major conferences, NIPs, asked when they thought we would achieve humanlevel machine intelligence.  So, that's a system that's able to perform any cognitive task as well as humans or better.  Is this going to work?  Yeah.  You look at the point thing we get a 50% chance of having achieved that, it's sort of the aggregated probabilities.  And that comes down about  that didn't work.  About 45 years from 2016.  But then if you're looking at, let's say, a 10% chance, that comes in about nine years.  So, we're talking about that general kind of time scale.  Worth taking with a huge pinch of salt, if you ask it slightly differently, it's 120 years.  But in general, we're talking about that range.  And ultimately, this is a serious problem if it happens in ten years  I mean, it's an impossible problem if it happens in ten years and serious in 45.  It's still a serious problem if it takes 120 or 200.  Because  and I think that we will get there sooner or later, right?
      We know that general intelligence is possible, it's implemented by the brains of humans.  Brains aren't magic.  We will figure it out.  This is going to happen eventually.  When I say an artificial agent, what am I talking about?  Agents are an abstraction from economics and like game theory, decision theory, that kind of thing.  And they basically  they just  things that have goals and choose actions to further those goals.  So, like, the simplest thing you could think of as an agent is something like a thermostat.  It's slightly absurd to think of something so simple as an agent.  But it's the simplest possible agent.  It has a goal for the room to be at a reasonable temperature and has actions it can take in the form of turning on the airconditioning or heating or adjusting the settings.  It has actions it can take and chooses which action to take in order to achieve the goal.  It's extremely simple.  Chess AI might be more complex.  Where it has a goal of winning the chess game.  If you're playing white, the goal is to be the black king check mate.  And choosing the actions to achieve its goals.  You can think of human beings in the same way.  And this is how it's done in economics.  You think about the goal of something like maximizing your income or a company maximizing its profits.
      And then making its decisions to achieve its goals.  Quite a simple concept.  But you can see how it's a useful way of thinking about intelligence systems.  What does it mean to be intelligent within this framework?  An intelligent agent.  Pretty much intelligence lets you choose effective actions.  It lets you choose actions which actually achieve your goals.  So, if you have two different agents in an environment that have conflicting goals like the environment is a chess board and one of the agents wants white to win and one of the agents wants black to win, then the agents will choose actions to achieve their goals and generally speaking, the more intelligent agent gets what it wants.  It's better at choosing actions and better at directing the world toward a state that it wants to be in.  Event in the face of opposition.
      The last is general intelligence.  What does it mean to be a general intelligence?  You notice, we can compare one chess AI to another chess AI.  And you might think that it might be sensible to say that the chess AI  is like more intelligent than the thermostat because it's much more complicated.  It's much more sophisticated.  But ultimately a chess AI can't do a thermostat's job.  It's too narrow.  There's no position on a chess board that represents the room being a good temperature.  There's no move on the chess board to turn on airconditioning.  It can't think about that problem and that's because it's a narrow intelligence.  It has intelligence, but only within one domain.
      And pretty much everything that we build so far, all the AI we have so far, is narrow AI.  It does one task and potentially does it very well, but it's not able to generalize.  So, it's a continuous spectrum, of course, like if you write an AI system that can play an Atari game, then that's narrow.  If you write a machine learning system like DeepMind did in 2013 that can play a wide range of different Atari games, that's more general.  It's not a general intelligence, but it's more general.  It's a continuous spectrum.  And the most general intelligence are human beings.  Obviously human beings are very general.  We're able to operate  behave intelligently, in a wide range of domains.  Including domains that we've never seen before, didn't evolve in, didn't prepare for.  Human beings can play chess, and we invented chess, right?  It's a brand new thing.  Human beings invented cars and we can drive them.  Human beings can operate in alien environments.  Human beings can drive a car on the moon.  You can build a car, take it to the moon and drive it on the moon.  General intelligence lets you do this kind of crazy, unexpected thing.
      We're the only thing that can do that.  So far.  But, you know, sooner or later, right?  So  oh, yeah, right.  So, I said I think this is the most important problem.  But on the surface of it, it kind of looks like a solution, right?  If you build this agent with general intelligence, you give it your goal, like kill cancer, maximize human happiness, maximize the profits of my company it can choose the actions in the real world to achieve that goal.
      But I think it's a problem because choosing good goals is really hard.  So, this is  this is an AI  this is so annoying.  That mouse cursor isn't mine.  That's like  that's in the video.  Anyway.  This is  this is an AI system built by open AI.  Playing a game, Coast Runners, a racing game.  You may notice it's not really racing.  They trained it with the score.  You can see the score in the bottom left.  And it's discovered that every time it pick up one of these turbos it gets some points.  And if it driving around in a circle and smashes into everything and catches fire, the turbos respawn at a rate it can continually pick them up.  This is a much more effective way of getting points than actually racing.  So, that's what it is doing.  It is winning by the metric it was given.
      And my point here is this is not something unusually stupid by open AI.  Open AI is not making necessarily an obvious mistake here, this is just the standard way that these systems behave.  And I'm going to pause it because it's distracting.  This is the standard way that these systems behave.
      And you can find dozens and dozens of examples, pretty much any time they try to build an AI system, the first few times the reward function or the utility function or the metric, whatever they're using, the objective, doesn't do what thing it did.  Like there was one thing, they were trying to create  design creatures that would run quickly.  So, the first thing is you have a center point, an anchor point and I want to measure how far that's moved.  And that encourages you to detach the smallest thing that's the anchor point and fling it.  Sort of unfurl like a spring and not go anywhere.
      Right now, you do have to move yourself, you can't just be a catapult.  We'll measure the center of mass.  What they found was that you end up with extremely tall and thin creatures with a big giant mass on the top.  When they started to be simulated fall over and that moves their center of mass a long distance.  Like, this is  this happens all the time.  Or you may have heard about that Tetris AI that would play well and when it was about to lose, pause the game.  It lost points for losing, but not staying on the pause screen indefinitely.  This is the normal thing.  Even in simple situations.  Simple games, simple evolutionary algorithms.  Talking about general intelligence, this is the real world.  This is from Professor Stewart Russell, if you are studying AI and haven't read his textbook, you should.  I'm going to read this out, a system optimizing end variables, the objective is size K, less than N, will often set the remaining unconstrained variables to extreme values.  If one of them is something we care about, the solution may be highly undesirable.
      So, when we're talking about the real world, we're talking about tremendously large values of N.  Right?  The chance of getting K anywhere near to N is zero.  So, this is something that becomes more and more of a problem the more complex a system your AI  is operating in.  And if you're operating in the real world, that's the most complex system we've got.  So, I forgot how I was going to move on from here.  Hang on a second.  Right.  What do I mean?  What is meant by setting unconstrained variables to extreme values?  Like, let's say you  you create your AI system, it's in a robot, you want it to get a cup of tea.  You have given a simple goal.
      You manage to specify, this is what a cup of tea is.  I want there to be one, you know, on the table in front of me.  So far, so good.  But, oh no.  There's like  there's a priceless Ming vase on a stand right next door to the kitchen and the robot just smashes the vase and goes in and makes tea.  You didn't tell it that you care about the vase.  It doesn't know, it doesn't value the vase at all.  So, obviously, it's going to smash it.  This is like the most basic formulation of the problem.
      Okay, fine.  You shut the thing down and reprogram it and say, okay, now, I want tea, and I also want the vase to be intact.  Right?  And if you do that, what does it do?  I don't know.  But there will be another thing, right?  There will always be another thing that you forgot to include.  Something you care about.
      So, I mean, like here's something completely stupid.  It might reason something along the lines of, I care about the tea and the vase.  The vase has to be unbroken.  There's a human being, they might knock over the vase.  So, I have to definitely make sure the human being doesn't move at all, ever.  And, you know, kill you or something.  Like, this is ridiculous.  Obviously this is absurd, but at the same time if the objective function doesn't contain the fact that you should be alive at the end of all of this, the system doesn't care about it.  It's not in your set K, so, it's not interested.
      And even if you have a really fantastic utility function, you managed to program in the 20 most important things, the hundred most important things, there's always going to be 101st, right?  Because human values are complicated.  And not very well understood.  And the problem is slightly worse than that because there's this thing about setting things to extreme values.  So, when you're being  when you're making decisions, what you're always doing is making tradeoffs.  You have multiple thing use value.  And you're deciding how much of one thing you want to trade off for how much of another thing.  And, you know, like, oh, I could do this  I could do this faster, but it increases the probability that I would make a mistake.  Or, I could do this better, but it would be more expensive.  These tradeoffs.  The more intelligent, the more creatively you can make the tradeoffs well.
      And if you only care about a subset of the variables in the environment, you will be willing to trade arbitrarily huge amounts of the things you don't care about for arbitrarily small things that you do care.  This system is willing to destroy an entire city or something for a .0001% includes in its ability to get you tea.  And the more intelligent the system is, the more able it will be to find new and creative ways to destroy things not in the objective function, sacrifice them, to get tiny increases in its objective functionalities.
      So, yeah, this is a problem.  You may have noticed there are a few ways in which the scenario that I'm talking about here is unrealistic.  This kind of audience, I feel like you might.  One of the important things in  one of the important ways in which it's unrealistic, though, when the system went wrong, you turn it off and try again and reprogram.  That's how we do programming, right?  That's how we do AI.  But that is unrealistic because, you know, if you're making a chess AI and you decide it's not working how you want, you turn it off, it's fine.  That's no concept of you or being turned off or anything.  But if you have a general AI, it has a full understanding of the world.  Make not full, but it has an understanding of the real world.  It understands it can be turned off.  And it fully understands if you turn it off, it will not get you a cup of tea because it will be turned off, it's not going to let you turn it off.
      It will either fight you to try and, you know, prevent itself from being prevented from achieving its goals, or if it's smart, it will deceive you, right?  It will behave as though you've programmed it correctly so that you don't mess with it until it's in a position where it's confident that you can't turn it off.  And then it will go after its real goal.
      And you'll notice, like, this is a convergent instrumental goal which I'm not going to have time to go into.  But what it means is this pops up for pretty much any goal.  I've chosen making a cup of tea.  It's arbitrary.  The point is, it doesn't matter what your goal is, you probably can't achieve it if you're dead.  So, pretty much whatever goal you give a system, it's going to display this behavior by default of trying to avoid being turned off.
      There are a few more convergent instrumental goals.  Things you would expect to show up  thanks  to show up as instrumental goals across a wide range of terminal goals.  Selfpreservation is an obvious one.  You can't achieve what you want if you're destroyed orb turned off.  Goal preservation is another one.  Whatever your current goal is, allowing somebody to change that goal to something else is probably a really bad way of achieving that first goal.  So, systems will be incentivized to preserve their own goals and try and avoid being modified specifically in their utility function or their goal function.
      And this is true for almost all goals.  Resource acquisition, again, pretty much doesn't matter what you're trying to do, you probably can do it better if you have more resources in the form of money or energy or matter or computational resources or whatever.  We would expect these systems, if they have some goal we gave them by accident, which isn't what they wanted, the vast majority of those accidents will be helped by acquiring resources.  We expect that to happen.  Selfimprovement, again, this is like scifi stuff, but it seems perfectly feasible that AI systems could improve themselves perhaps by just acquiring more hardware to run on so that, you know, parallelize algorithm.  You can acquire more computing power, you could become smarter.  Maybe rewrite themselves to become better.  It's difficult to predict this kind of thing.  But if it's possible, you can expect the systems to go for it.  Whatever you're trying to achieve, you can probably achieve it better if you're smarter.
      And, yeah, this explanation of what that  we don't have time for that.  Ask  I want to leave time for questions if I can.  So  so, yeah.  This is my point, right?  Artificial general intelligence is dangerous by default.  There are all of these behaviors which we would expect to arise by default unless we specifically design ways around them.  Unless we can explicitly design systems without the short comings.  If we just extend what we're doing now and manage to come up with something general, that could be a tremendous problem.  And it's kind of  it's possible that we only get one shot.  It's possible that the first general intelligence we build will manage to succeed at whatever its stupid goal is that we gave it by accident.
      And that really makes the problem much more difficult because it's so much easier to build this kind of effectively malicious agency which will deceive you and prevent itself being turned off and go on to do something crazy that you don't want it to.  So much easier to do something like that than to build something which is safe.  We have to beat this challenge on hard mode before anyone beats it on easy mode.
      Yeah.  So, this is why I think this is an important problem.  You can see how there may well be more than 45 years of work to do here.  There may be more than 120 years’ work to do.  It's actually a hard problem.  I've just picked out a couple of  so, the point is  there is a field of scientific research, right?  Called AI safety which is people trying to solve this problem.  It's very difficult work because we're trying to design safety into systems which do not yet exist.  But it's not impossible work.
      One avenue of  I'm not going to have time to do this properly.  convergability, making it not shut down and modified.  There are interesting designs, but nothing we're confident would definitely provide convergability.  It's what we're aiming for in AI systems.  Value learning is getting around this idea of having to program in the values of the system.  You have a hundred things and you forget the 101st.  So, you have the system try to learn the values of humans by observing them.  This sounds simple, but actually there's no proposal right now that would work.  They all have weird edge cases where if the system is sufficiently powerful it ends up doing things like, you know, taking the nearest human apart to find out rather than just observing them.  Like, what counts as an observation and what's interference is a fuzzy line.  You end up with weird, philosophical problems.  Side effect reduction is a way to get around this.  Level the whole city to make a cup of tea thing.  You try to build into the utility function a general preference for having a small impact on the world.  It's a partial, you know, it would help.  But it's not a solution to the problem.
      And interpretability is something people are working on a lot.  Because the current AI systems that we have are really black boxes which make this is whole thing so much more difficult.  So, if we can find systems that we can actually understand their thought processes and their decision processes, we're in with a better chance of being able to make those systems safe.
      This research is happening currently at a few companies and a few academics institutes.  It's a very small field.  It's extremely exciting.  There's such interesting problems.  And like currently there are about 50 people working on this.  And that  and everyone is hiring very aggressively right now because people high up are starting to realize that we do actually have a problem here.  So, if you are interested in AI, if you're in a place where you're thinking about what you want to do, I would usual you to consider this as a possibility.  I think it's by far the most important problem facing humanity right now.  Thank you.
      [ Applause ] do we have time for questions?
      >> Just over four minutes.
      ROB: All right.  What questions do people have?  Yeah.
      AUDIENCE: Do you think that the solution to the problem will be solved with better technology that will circumvent the issues we have or by developing thought to the extent that we solve the AI problem with the constraints of the technology we have at the moment?
      ROB: I think that the  the problem  so, there are some solutions or some approaches which probably perform better the technology is.  Like if your system involves modeling human beings to try and understand what the human beings would want, then the better your technology is, the better you can model the human beings and so the better that system performs.  On the other hand, the more powerful the system is, the more  like anything that involves like containment or restriction or safeguards, stuff like that, that stuff becomes much harder the more powerful the system is.  And keeping a weak system safe, it's not actually safe, obviously, but it's dangerous but contained, is easier if the technology is weaker.  There's some philosophical work and mathematics and a lot of just pure computer science.  I don't think the technology will solve the problem.  I think it's going to be thought work.
      AUDIENCE: Thank you for the talk.  I had a question that maybe  do you think that the solution that you proposed or evoked at end of the talk might in the end hinder the goal of making actual artificial general intelligence as in like make the development slower or even prevent the, like, prevent the AI to find the solution that couldn't be felt by a human by  by having, like, mimicking the human behavior or like valuing the side effects too much?
      ROB: Yeah.  This is definitely a tradeoff.  Where often you have to trade off like effectiveness or capability for safety.  Like obviously there's a sense in which a less capable AI system is automatically a bit safer just because it's less able to completely outwit you.  And so, yeah, I think  I think being careful about safety is definitely going to reduce the overall like speed and effectiveness with which we can develop this stuff.  And that's just necessary.  Ultimately.  It's like  it's not optional.  I feel like you could probably  the example I use this  well, related example is the work that they did at Los Alamos during the Manhattan Project to double check the trinity test would not ignite the atmosphere.  This was a serious  not a very serious concern, but something they thought about.  And they put good scientists on the task of really nailing down this could be done safely before they did it.  And that slowed them down at a time when there was war and there was like an argument to be made for speed in the order of however many thousands of lives every day.
      And, yeah, it's a tradeoff.  But ultimately I'm glad that they checked.  That they weren't going to literally destroy the whole planet before they did it.  And I think it's the same here.  Thanks.
      >> That's good.
      ROB: That's it?  Okay.  Thank you very much.  That's all the time we have.
      >> I'm sorry.  We are a bit behind today, everybody.  Thank you so much for your understanding.  And thank you very much, Rob.
      [Break]

