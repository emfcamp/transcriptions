Codes of Ethics for tech: the hip old thing
      Catherine Flick
      
      CATHERINE: Thank you, my dear friends here.  We have been friends for a long time.  It's all good.  Hi.  I have to apologize a little bit for the state of my slides which are a little bit quick because I have been trying to fight fires most of today trying to get the stages actually moving.  So, please apologize  please accept my apologies.  I'm basically here with two hats on.  I'm a technology ethicist and on the committee of professional ethics for the ACM.  Some of you may be ACM members.  Don't worry, no hands up or anything.  Some of you maybe know what the ACM is the association of computing machinery.  The largest professional organization for computing.  Anyone who does anything with computing.  Computer scientists, games designers, game art people.
      Even like we have like financial service people.  It runs the whole gamut of tech.  And what we've done recently  oh, yeah, before we start, if you have questions, I probably won't get to them.  But I could look at them and possibly write about them later.  Slido.com, open up your phones and put that in and then  just the join code is EMFcode.  And you can put a dilemma or a question in there and it may get turned into a case study for the code of ethics if it's  we'll anonymize it, but it may turn into a genericstyle case study.  If it's an interesting dilemma.  Or if you want to talk to me afterwards.  Or if I have time, I'll go through the best ones.
      Okay.  So, the code of ethics.  Like I said, the ACM is a really big computing organization.  And we had a code of ethics.  It was a sort of code of conduct before 1992 which basically said these are things you shouldn't be doing.  And then in 1992, they decided to sit down and write an actual code of ethics.  And what then happened is they said, oh, we'll update it frequently and then they didn't.
      And then the Internet came along and AI came along  well, machine learning as we know it currently came along.  And a whole bunch of other things came along and then it got to 2018 and they were like, yeah, we probably really do need to update it now.  Over the last two years I've basically been part of this process.  I was on the steering committee for updating this code of ethics.  It was a huge participant experience.  We got loads of people to comment on drafts.  It's pretty much ground up built but based on the original code.  There are some similar things and some new things.  I'm not going to expect you to have read the code.  What I'm going to do, though, is take you through a few things that might be useful to look at.  If you're an A CM member, one of the conditions of your membership of the ACM is that you abide by this code.  So, you probably really want to have a listen to this.
      Yeah.  You also probably got a bunch of emails from us saying, please help.  Thank you if you did help us.  So, why do we need something like this?  It's really interesting, as a technology ethicist, I have been doing this for a really long time now.  Like 10, 15  a really a long time.  Longer thank I would like to think.  And it was up until about a year ago, I was the one knocking on people's doors saying, you need to think about ethics.  About a year ago something happened, Cambridge Analytica and a bunch of AI stuff that people got worried about, and I got all these invitations to come at particularly AI conventions, like big industry conventions.
      Now it seems like the flood gates have opened.  I would like to be out of a job at some point because then we'll have solved all these problems.  But basically there seems to be a lot of call for ethics right now because people are realizing that technology can have some significant social harm attached to it.
      So, one of the key focuses that we really wanted to put into the new code of ethics was a focus on the fact that we should be using our skills at technology creators, software developers, et cetera, innovators of any kind that has anything to do with tech, we should be using it for the public good and we should be using it for the social good.  There's no point in making tech for tech's sake if it doesn't actually have some benefit to somebody somewhere.  It's about things like benefits, looking at the benefits versus the harms which I'll get into in a bit.  But the key thing is the first sentence of the preamble, computing professionals actions change the world.  To act responsibly, they should reflect on the wider impacts of their work, consistently supporting the public good.  Blah, blah, blah, expresses the conscious of the with profession, mostly because it has been a ground up activity.  We wanted to make sure this wasn't just a bunch of people from ivory academic towers coming down and preaching the word to the people.  We wanted to make sure that actually everybody was involved.  We wanted to practice what we preached, right?  We wanted this to be relevant to people actually doing things in the real world.  So, the very first principle that we have is that a computing professional should contribute to society and to human well-being, acknowledging those in computing.  It's no longer acceptable to create things in a bubble.  Especially if it's going to have some sort of social impact.  You need to be out there making sure that you're doing it with the public good in mind and also collaborating with stakeholders.
      Anyone who might be impacted by the technology that you're creating.  So, a couple of the major changes that happened between 1992.  I mean, we had the Internet showed up.  We had a whole bunch of awakenings.  And I think the previous talk was a good example how things have shifted over the past 20, 30, however many years.  We're now much more aware of women's role in computing, computer science and like there's still a long way to go there before there's kind of equal treatment of women in computing area.  Although there's a lot of work being done, which is fantastic.
      So, there's a much broader description of discrimination and what that entails.  So, before the code didn't have anything about, say, harassment, particularly sexual harassment.  That's now specifically in the code.  And it's basically  there are also a whole lot more aspects of like discrimination that are covered.  So, we used to just kind of have race, gender and something else.  But now we have a whole bunch of other  other aspects that are now covered.  There were two big things on intellectual property rights.  So, it used to say basically you must abide by intellectual property rights which I'm sure many of you here would be horrified to think that was required to do.  And me too.  So, I was given the homework as part of this thing to redraft the intellectual property section.
      Which basically turned into a respect the work required to produce new ideas, inventions, creative works and computing artifacts.  Respect the decisions that creators make within reason.  And there are basically  if it's ethically justifiable to break these things, you have an out in there as well.  There's a way to get around that now.  Yeah.
      Anyway.  Design and implement systems that are robustly and usably secure.  Back in 1992, there was very little security apart from physical security.  Now we are requiring security by design.  Which is something that most people hopefully do anyway.  But it's something that you now actually are required to do by the code of ethics.  3.6, use care when modifying or retiring systems.  This was certainly not on anyone's radar back in 1992.  But thing like Microsoft retiring certain operating systems and various other largescale things, Google Reader comes to mind as one that really annoyed me.  Basically, if people are using your software, you have to have a really good reason to retire it.  And then you also need to help shepherd them on to something that will basically replace the thing that you're retiring.  3.7, take special care of systems that are integrated into society.
      Facebook.  Twitter, Facebook, if you're creating something that then goes on to become not just that thing you do in your garage or with a couple of your mates, but is a massive infrastructure thing, you have to be careful how to handle that.  And there's guidance in the code of ethics how to do that now.  So, how do I use this practically?  Well, the idea is basically it's supposed to be a way for you to reflect on what it is that you're doing.  It's not a step, you know, a howto do things ethically.  There is no onesizefitsall rule for any of this stuff.  Ethics is not a set of rules.  It's a way to get you to think about what it is that you're doing.  There are no easy ways out in ethics.  You need to actually sit down and think about it.  So, I'm going to take through a few examples of some things.  So, this is slightly adapted from a video games conference that I went to recently.  But I've tried to kind of adapt it to be a bit more general.
      So, I'm going to take you through just a couple of ways how you might think through these issues.  So, if you're creating technology for vulnerable people, so, children, old people, anyone with various disabilities that might be  make them be considered vulnerable, there are a whole bunch of things that you can think about that the code will help you think through.  For example, there's a whole section on avoidance of harm and what exactly is harm and how you can, like, you know, make sure that the technology that you create doesn't inflict this on people.
      I'm going to get a little bit of the complication to the harm thing which I'm going to get to in a bit.  If you need honest and trustworthy, particularly with vulnerable people.  This is because they don't often  sometimes there are reasons why they're not able to make very good decisions for themselves.  I have the picture up here of premium children's games in the mobile game industry.  Basically that's one area where children are not able to consent to say, hey, pay money for games and you need to think about that when you're developing it.
      Fostering public awareness and understanding of computing, related technologies and their consequences.  Principle 2.7.  Basically this asks you to do that people understand what it is you're creating.  And yes for the machine learning people, this is complicated, but you need to have a go.  And if you struggle with that, I do a lot of that sort of thing.  If you want to talk to me at some point this weekend, I'm more than happy to help you explain, you know  I can explain Bitcoin in 15 seconds for the local radio station.  I'm quite good at simplifying things.
      2.9, design and implement systems that are robustly and usably secure.  Security by design.  Protect people.  Can't just throw stuff out there that's not got security by design in it.  Another one, data analytics.  So, basically discrimination, obviously, is a huge  people are worried about bias of algorithms.  The fact that a lot of the data that is put into the training for these systems is biased to start with.  How do you remove bias?  This is an ongoing question that machine learning people need to think about particularly.  And data analytics people need to think about.  And it's not that there's a solution in the code, but it basically says you need to think about this and you need to have really good reasons about decisions that you make about what you include and exclude and anonymize and all that sort of thing.  Respect privacy hopefully is selfevident.  Honor confidentiality.  This is an issue about basically if you have confidential information, be careful how you treat it.  That's data analytics, some of the information you may dealing with is confidential.  Processes and products, professional work.  This basically has a section that says if you don't understand what your analytics is doing, if you don't understand how your machine learning algorithm is making decisions, then maybe that's not something you should  I know this is a really complicated area.  So, I don't want to oversimplify it, but you need to really think about what it is that you're actually doing with that algorithm and how it might work and what information actually might be those key kind of things that are then used for decision making.
      And that's part of the quality and testing.  And I've got a picture of Tay bot who became a horrible person on the Internet quickly.  This principle says you need to test things better.  If you're not able to test them sufficiently beforehand, you need to monitor them out in the wild doing their thing and pull it back if it's doing terrible things quickly.
      Oh, my laptop's just turned off.  Respect existing rules for work.  Stay in the law where you can.  There are exceptions for if the laws  we have a lot of exceptions for basically we're worried about kind of fascist regimes and things like that.  This is something in 1992 we weren't so worried about.  But we are now.  If the rules are unethical, there are ways of challenging the rules.  And there's way in the code to do that.  This should all be done with the public good.  I'm going to skip over diversity.  Hopefully  it's mostly just for time because I want to get through the rest of this stuff.  You see what I'm doing, I'm using aspects of the code to get to basically get questions up in your heads about perhaps what it is that you're doing to challenge what it is that you.  And some of that might be uncomfortable.  And that's totally fine.  But it doesn't mean that you can ignore it.  That means that you need to be something that you start thinking about integrating into how you do your work.  That sort of reflection  that reflection time.
      And I'm going to talk about that in a sec.  So, some frequently asked questions we get at the Code Central.  Which is ethics.ACM.org.  What if my boss thinks codes of ethics are for loses?  Basically, what if your boss asks you to do something against the code of ethics?  If you're an ACM member, you can wave this at them.  If they say, screw you, there are actually ways of going through whistle blowing and stuff.  And the ACM is  we have to be very careful about how we promote that aspect of the code because although it's been used for legal decisions and things in the states, it's actually very difficult obviously for an organization to help support people all around the world.  But we're able to give people guidance, but ultimately unfortunately when it comes down to it, the rules of the land are the rules of the land.  You have to understand that you might be break those in some ways, right?  Yeah.  So, another question we get, what if I'm in the military or security, does that mean that the code of ethics is not for me, and I may be harming people and the public good may not be the center of my concern?
      And basically we have been very careful to word the code so that if you are in the security or military domains and you do things that might be considered slightly eh  that there are ways of thinking through the code that actually can keep you in this process.  So, for example, in the harm  the avoid harm we changed that so many times.  You hear about do no harm, right?  And that's kind of like the Hippocratic oath.  It's about ethically justifiable harm.  Sometimes in security, there may be some ethically justifiable attacks you need to make against a system or something like that might be considered otherwise against the code of ethics.  Why didn't I plug this thing in?  Or if you're in, like, weapons development or something like that, there might be times where you  that your ethics might be better spent sort of working on thousand make it so that other people aren't harmed by the specific weapon.  These are all things we had to think about and it's really, really complicated and we're never going to please everybody.  Although I'd really  I'm a pacifist.  This stuff is really uncomfortable for me to talk about.  But I can kind of see the point that basically we want to keep people thinking about this.
      We don't want people in these sectors particularly to say, well, this isn't for me.  Basically, as far as I'm concerned, if the code gets them thinking about this sort of thing, actually maybe military or bad security isn't actually for me, then that's a good thing as far as I'm concerned.  But, you know, there are kind of  these very fine lines are very difficult to walk along, I have to say.  Come and see me at the bar after I've had a few drinks and I'll tell you more.  How is this code different from other codes?
      So, some of you may know the IEEE software engineering code.  It's different because firstly it's newer.  And also it's a much more general, and as an aspirational code.   The software engineering code is much more a list of don'ts.  The ACM code is aimed at students to inspire them to do things well as opposed to slapping them on the wrist if they don't do it.
      But speaking of that, what if I break the code?  If you're an  if you're not an ACM member, nothing, unless you break the law, that's a different thing.  If you are an ACM member, we can discipline you, which sounds scarier than it is.  But also we have  that's not what I wanted to do.  There we go.  Also basically the committee  it's very complicated, but we have a much better system now if someone  or you think someone has broken the code of ethics that might be an ACM member.  The committee on professional ethics will take it on as a case and we kind of deal with it.  And there are basically a range of disciplinary actions that we can take including stopping you from coming to our conferences.  And that's probably about the worst one you'd get from the ACM unfortunately as far as I'm concerned.  But anyway.  Yeah.
      So, that's basically  yeah.  If you break the code and you're an ACM member, that's not good.  All right.  So, previously I talked about integration of this sort of stuff into your ways of working.  So, how can you actually do it practically?  Well, there's another thing that we do in Europe called responsible innovation.  And this is actually my other hat, my academic hat that's now on.  Responsible innovation is basically the idea of creating technology with and for society.  And the  this is ethically aligned.  The idea is to get people thinking about ethics and getting them to integrate into the ways that they work with the innovations that they make.  So, this area framework is the one that we mostly use in the UK.  And in parts of Europe as well.  But basically it's about getting you to  asking you to anticipate what the potential impacts of your  of your technology or your innovation might be on society.  Reflecting on the ethical and social issues that might raise.  You can use the code for that.  That's a good place to use the code of ethics.  Engaging with relevant stakeholders to identify potential issues and mitigate those.  And not just any testing stage, but the very beginning of the innovation cycle.
      And then act by putting methods in place to ensure issues are resolved.  This is like having good HR policies and processes, having good tests QA.  That sort of thing.  Codifying a lot of the good practice you do already into methods that you follow every single time.  And hopefully these are responsible ones.
      So, benefits to behaving ethically.  This is left over from my video games talk, which is great.  I'll leave that one there.  So, we have been doing work with companies.  I have been working with cybersecurity companies in the UK for the last go years getting them thinking about responsible innovation and ethics in what they do.  And they've talked to me about what they feel is the  are the benefits that they have been getting from this.  Basically it's all about trust.  They have better reception from the public.  They have better reputation which builds trust.
      They have employee satisfaction, which builds trust within the employees that they have because like to work for ethical companies.  They have better quality innovations because people are stimulated to work and do things better.  And you get a wider audience than just nerd bros.  Sorry for all the nerd bros out there, but you are the minority.  And there's a broader audience for all sorts much technology than just the power users.  Sorry, I'm not equating power users to nerd bros.  They're different.  Not doing that.  And you're doing the right thing so you can sleep at night.  So, that's nice as well.
      So, more on ethics and technology.  Basically there's a bunch of links here.  If you just Google ACM code of ethics, you can find the ACM code of ethics.  We do a podcast on video games, and she's been doing a fabulous job on stage B all day.  Thank you.
      And also that responsible innovation stuff that I talked about is available at orbitRRI.  Which is the main center for the UK's responsible innovation information stuff.  And you can find me on Twitter and the ACM ethics stuff is on Twitter.  And we have done Reddit stuff.  We're trying to get the word out.  And if you would like a sticker that has ACM ethics, like that one, come down to the green room and I will give you one because I was stupid and forgot to bring them up.  All right.
      So, got a dilemma.  How much time have we got?  We got some time, I think.  We have time.  Okay.  Good.  All right.  I'm going to refresh my slide and see if anyone's put a dilemma.  Fantastic.  Okay.  We're at the peak of height for tech ethics, when it's out of the phase and it's less hip again, what's the biggest thing that will have changed in the world?  I think actually what will probably happen is that tech companies will have much broader stakeholder work.  Like they'll work much more with the general public in ways that are much more inclusive and not just people who are white middle class people who can afford to go to a one day workshop and try out a new piece of technology.  It will be ways to include people perhaps of lower socioeconomic status who don't perhaps  who may only use the technology in kind of emergency situations.  Who were much more removed from the direct use of technology.  And I think that will actually really improve a lot of  that will be the biggest thing that will be changed.  I'm not quite sure if that's what you meant in terms of biggest thing that will have changed.
      There will probably be deaths of some big technology companies in some ways.  Anyway.  I wanted to mention how Google  the Google employees pushing back against the Chinese censorship used the new ACM code of ethics as a basis to push back to their employer.  I wrote a piece in the conversation about Google and  Google in China and how the ACM code of ethics thinks that's a bad idea and they used that as like  which is really nice.  So, I was quite happy about that.
      All right.  I just  anonymous asks.  You're all anonymous.  I completed a computer science degree, what can we do to change this attitude?  Employee ethicists to teach the ethics courses.  Not just people who  like often in computer science departments it's left to someone who has spare time.  Or they mentioned that they were interested in ethics once and now they're lumped with all that teaching that isn't computer science.  I know in some departments  you need it to get BCS accredited.  The British computing society requires ethics in your program if you want to get accredited.  And I have known certain departments which I will not name say it's something that's taking away from teaching students tech and that's a bad thing.  And so, it has a negative lump that comes along with it.  Even at the top level.
      So, it's about changing attitudes.  Like with the cybersecurity companies I've been working with, the only way you can get companies to take on kind of an ethical culture is to change the minds of the people at the top otherwise it really doesn't  nothing  I mean, unless you've got a really big workforce and they're all aligned with being ethical which rarely happens, you're going  it's difficult to push back against a CEO or a department head that doesn't want to do this sort of thing.
      Good.  They're coming in quick and fast.  Do let me know when I'm out of time.  Two minutes.  Okay.  What should Facebook do to act responsibly about their role in society, specifically with advertising data internally?  Specifically in relation to that, they should probably delete it.  But, I mean, I think they have  they're really doing a lot of soul searching at the moment, which is a good thing.  I still wouldn't have a Facebook account.  Yeah.  I read about that too.  I wrote a thing about informed consent on that.  And it was very interesting.
      Anyway.  What should they do?  So, what Facebook didn't do, let's just say  let's talk about what Facebook didn't do.  What they didn't do is they didn't establish a method for reliably communicating consent aspects when it came to the data that they were collecting.  So, they weren't very good at communicating what data they were going to collect and how they were going to use it.  And they also didn't do a very good job of managing the kind of fall  basically did a very poor job of managing the kind of political aspect of that.  So, actually there's a really good radio lab if anyone listens to the radio lab podcast on Facebook and how they do a lot of their content moderation, for example.  And the fact is they have all these rules about what can and can't be on Facebook.  And they're just like  the problem with ethics, like I said before, is it's not a how to.  You can't step through and say step one, do this, step two, do that.  The thing is when you have a whole lot of context and you have to then make decisions about, do we allow this on our platform or do we not?  It usually comes with a lot of context.  As a worldwide company, you have a lot of worldwide context.  And it gets complicated because in China you can say things  you can't say things that you can say in the UK or America or the Netherlands or wherever it is and everything gets very complicated.  I think and they really did that very poorly.
      And in some ways it was kind of inevitable.  But the way that they can solve this problem now is probably, to be honest, to really kind of scale back their operations.  Certainly on the political side.  Really focus on kind of going back to what it is that they actually wanted to do, which was to be a way for people to communicate with their friends and family.  But then also  and this is sort of walking a fine line  just be very careful about what sorts of content that those sorts of communications actually have.  But that needs to be something that's agreed with the community as well.  Like they need to build it up with certain groups.  And it gets very, very complicated.  Come see me at the bar.  That would be a good one.  One more or are we good?
      One more.  All right.  Laws are slow to change and adapt.  This is a good one.  Laws are slow to change and adapt compared to technology.  What about the groups unwilling to follow the code?  This is a classic more policy vacuum.  And this is something that technology's just  we've all, like, this is going back like many, many, many years we have been dealing with this.  What we can do is we can do things  we can be good examples.  We can show the benefits of doing things ethically.  So, these cybersecurity companies, for example, who have been doing this.  You know, like they are now being like  they're sort of being held up as kind of pinnacles of good tech.
      And we can also, you know, vote with our, you know, with our wallets, right?  So, don't get a Facebook account, you know?  Don't sign up to the latest thing just because it's there.  Actually have a look at what it is and what they want to do.  And also educate your friends and family that may not be as tech savvy as you are.  That's certainly a big thing you can do.  Help them make informed decisions.  Because at the moment they're not able to really do that because they're not being informed very well.  Okay.  Thank you very.
      [ Applause ]
      I can do more of these in the bar afterwards.  Actually I think I'm going to emcee the next talk. 
      [Break]

