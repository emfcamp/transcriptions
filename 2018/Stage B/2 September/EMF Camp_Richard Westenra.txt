      Actually, it's about ethics in software engineering
      
      Richard Westenra
      
      >> Welcome Richard Westenra who is going to  in software engineering. 
      [ Applause ]
      RICHARD: Hey, everybody, I'm Richard.  I originally wrote this talk a year ago for an audience of web developers, but it applies to anyone in tech or engineering disciplines or ethics in tech.  High level, there are no  there are some bad coding jokes.  Anyway, I named this talk the good life, but actually it's about ethics in software engineering.  A bit about my background, I started out at university studying engineering, mechanical engineering, which I hated.  And I switched to velocity which I loved, and now ten years later my job title is frontend engineer.  So, I'm a frontend engineer these days, but I refer to myself as a developer.  I have felt uncomfortable with the title of engineer.  I couldn't put my finger on why.
      Maybe I was claiming prestige I didn't earn.  I didn't study computer science, I'm good at my job but I'm hacking things together.  Back in the day, many of us who built websites were called webmasters or web mistresses which I think is an amazing job title.  I'm sad I didn't start to be a webmaster in my CV.  People use it as an insult, but frankly, I think it's bad ass.  We were webmasters and web mistresses and they fell out of fashion, and we're web developers.  And now many of us have the professionalsounding title of engineers.  What do you want?  You can be a techno biking, or you can be riding a full stack unicorn, one there.  But we were working on dinky websites thrown together and working on massive financial operations on the financial backbone of multibillion-dollar corporations.  We have come a long way from webmasters and web mistresses.
      But a lot of us have selftaught with no standardized training.  And don't get me wrong.  This is great.  It helps ease barriers to entry which I think is a very good thing.  But as the industry gets more professional, it might be worth worthying about trappings of engineering.  And there's ethical duties and codes and responsibility that go along with that title.  A hundred years ago, civil engineering was in a similar situation to how the tech industry is now.  As the industrial Revolutions were behind them, engineers found the ways to use the fancy new technologies they had developed.
      they grew more sophisticated in their approach, and their projects ballooned in scale and complexity.
      But as these projects became more ambitious, there was an accompanying problem.  A rise in major engineering disasters.  The turn of the 20th century saw a wave of epic structural failures including some massive bridge collapses.  And also, the great Boston molasses flood, this is my favorite disaster of all time.  Just the mental image of a tsunami of sugar, 50 feet high, 50 miles an hour, consuming everything in its path.  It was terrifying.  Also, kind of delicious.  Anyway, these disasters had a profound effect on the way the public saw engineering.
      And forced engineers to confront their short comings.  So, as a result, they began to regulate themselves more intensely and established standardized industry codes of ethics.  What is ethics?  It was a branch of philosophy devoted to answering questions about what is best in life.  Questions like these.  And I know what you're all thinking I can see the cogs in your software developer minds turning over.  You're thinking, that's easy.  What is best in life?  Both spaces and tabs on alternating lines.  What is the good life?  It's when the client is banned from feature equips.  We should look outsourcing jobs to China and spend all day on rivet.  Interrupt people with headphones on, and the purpose of life is replacing everything with JavaScript.  You're all monsters.
      Philosophers like to do thought experiments.  They're like real experiments, but better because you never have to get out of the armchair.  One of the most famous is the trolley problem.  You are probably familiar, it's a common one these days.  There's a runaway trolley, and there are five people tied up and unable to move.  The trolley is heading straight for them.  You're standing offer in the train yard next to a lever.  If you pull the lever, the trolley switches to a different set of tracks.  However, you notice that there is one person on the side track.  You have two options.  One, do nothing and the trolley kills the five people on the main track.  Or two, pull the lever, diverting the trolley on to the side track where it will kill one person.
      So, which is the most ethical choice?  So, now here's the audience and direction part of the talk.  Quick show of hands, which of you would do nothing?  Okay.  And which of you would pull the lever?  All right.  Okay.  Cool.  So, most people pulling the lever.  Now, imagine instead of a switch, you're standing on a bridge over the tracks next to an extremely large man.  The trolley is coming, and the only way you can stop it is to push the large man on to the tracks.  He's the only one big enough to slow down the trolley.  If you jump on, it's not doing anything.  But he's looking you right in the eyes.  He can see what you're thinking and terrified and beg you not to do it.  What do you do?  How many of you would push the large man on to the tracks?  Fewer this time.  And how many of you would do nothing?  All right.  More.
      So, the trolley problem has been a subject of many surveys which tend to find that approximately nine out of ten respondents would throw the switch to kill the one and save the five.  However, the large man situation, the situation reverses.  And only one in ten people would push him on to the tracks.  So, that mostly corresponds to what's here.  Mostly in this crowd.  Incidentally, a 2009 survey of professional philosophers found 68% throw the switch, 29% had another view or could not answer.  If you're tied to a train track by a villain, hope that the person by the switch isn't a moral philosopher.
      So, why the difference in the two outcomes?  One theory is that it's because two different parts of your brain are fighting with each other.  Researchers looked at brains using fMRI machines and demonstrated that personal dilemmas, like the man off the foot bridge, engage regions associated with emotion.  Whereas in personal dilemmas, diverting by flipping a switch, is controlled reasoning.  And these different brain processes compete with each other whenever you have to make a tough moral decision.
      Basically, inside your brain you have a monkey and a robot.  Literally a monkey and a robot fighting over the controls.  Every time you have to make a moral decision, they duke it out.  The monkey understands something simple like pushing someone off a bridge and it's horrified.  But it doesn't understand something complex like a mechanical switch.  So, in that situation, the gut response is reduced, and we can throw the lever without a crushing sense of moral horror.  Some people have a stronger monkey and a stronger robot.  And that's great because they're both useful in different situations.
      But this is tricky for programmers, it's on difficult problems that makes it difficult for the monkey brain to trigger moral responses.  By the way, if you think it's hard for programmers to have the response.  Think for autonomous vehicles.  You can't make a perfectly ethical algorithm.  It's only as good as the humans who programmed it and we can't agree to use tabs or spaces.  There are really tricky problems that selfdriving cars face.  Like, we prefer a selfdriving car to swerve into a pile of trash rather than hit something.  Computer cans make the decisions quicker than we can.  If we decide in advance what we want them to do, they follow the instructions.  We want the car to hit a single adult rather than a bus load of children.  What if the adult is a Nobel Prizewinning cancer researcher.  What if they're driving the car?  Sacrifice the drive?  Would you buy a selfdriving car that's designed to sacrifice your life instead of others?  At MIT, they had a solution, they mined data on different answers to trolley problems to decide how autonomous cars should behave in different scenarios.  The website's called moral machine and you can go in and start judging scenarios.  Like this one, we have to choose between a male athlete driving a car and a jay walking baby.  On the one hand, the baby probably doesn't know not to cross on the signal.  But on the other hand, it might grow up to be Hitler, you know?  It's a cuff call.
      Moral machine is cool, but it doesn't help us all the time.  Because we can't outsource all of our decision making to the Internet.  We have ridiculous decisions, and we have to make the decision whether to kill baby Hitler.  We sometimes make the wrong call.  Let's shift gears and look at the emissions scandal.  You might recall that VW added special software to millions of diesel cars that would detect when their exhaust fumes were being checked by emissions regulators and change performance to pass these tests.  As a result, they managed to completely bypass emissions standards in the US, EU and elsewhere for a period of five years.  They emitted up to 40 tons more nitrous oxide than what US emissions standards allow.
      By some estimates, air pollution causes around 40,000 early deaths per year in the UK alone.  It's pretty safe that a technical hack is going to likely result in several thousands  or at least several hundreds  premature deaths plus thousands of cases of asthma and lung disease.  And as someone who started experiencing asthma symptoms for the first time since I was a child, probably because of London's air pollution, I take this a little bit personally.  When I heard that one of the engineers at VW was imprisoned for his role in the scandal I thought, good.  But on the other hand, I have to give credit where it's due.
      VW's defeat device is a pretty brilliant technical hack.  It's ingenious.  The engineers who created it must have felt pretty proud of themselves at the time.  But at the same time, you wonder why nobody spoke up at the internal meetings to say, hey, pals, maybe we're assholes?  How did they get it so wrong?  Are they just inherently bad people?  Maybe it's because the monkey part of their brain was unable to deal with the complexity of the problem.  You have cars and software hacks and air pollution.  And decades later some people you don't know might die.  It's all a bit much for the poor monkey to handle.  Ethical reason is a struggle for control, and humans can sometimes forget to act ethically when we're so focused on achieving a goal that we forget to think about the consequences or actions.  Or justify it to ourselves in ways that don't stand up to scrutiny and we never reflect.
      I'm sure we have all done this at some point.  I definitely have and led to some of my biggest screwups.  When you're looking at the code, it's hard to think about the humans affected by your decisions.  And it's easy to fix mistakes, roll out a patch or an upgrade.  In tech, I want to move fast and break things.  But we don't want to move fast into oncoming traffic and break people.  I think the monkey brain is in many of the ethical lapses in tech today.
      Whether it's Facebook enabling fake news and Equifax with their criminally sloppy security, or JavaScript developers too lazy to make their websites accessible to disabled people and keyboard users.  I'm looking at all of you.  And me too.  But I want to believe that the people making these decisions are doing so because they're not thinking hard enough about the consequences and the people that are affected by their actions.
      However, there's also those who say, I don't know about all that ethics stuff, I'm just an engineer.  It's not my responsibility.  Like Mr. Von Brown who couldn't be at the forefront of rocket research in Nazi Germany if he didn't go along.  And didn't know the crimes he had to turn a blind eye to if he could play with rockets.  Nobody is exempt from behaving ethically.  Scientists and engineers don't get to be amoral and don't have to think about this stuff.  Ethics is in everything.  Whether you're designing rockets or looking at gang members, you have a duty to determine how they might be used.
      With so many examples of ethically compromised decision making in tech, it's easy to get pessimistic.  There's good news.  It's easy for people to act ethically when they don't think about it, the flipside is that people tend to behave ethically when you remind them to.  And it can happen even in subtle ways.  For instance, researchers in New Castle found hanging up posters of staring eyes in a cafeteria significantly changed behavior and made people twice as likely to clean up after themselves.  If just a poster of eyes can achieve that much, imagine what you can accomplish with just a few wellplaced reminders.
      We want to establish an organizational culture where people tend to act morally and there are lots of positive examples for us to emulate.  And I think that our reminders are a powerful tool to help us achieve this.  I mentioned before that many engineering industry bodies introduced formal codes of ethics in the early 20th century.  These came along with more legal regulations and barriers to entry which I don't think is good for our industry.  The ethical codes are a great idea.  And that's already been touched on one talk this week  this weekend.  These are great way to remind to act ethically.  Because basically when you tell people, don't be a dick, they'll be less likely to be a dick.  We already do this with codes of conduct at conferences and other events, including EMF Camp.  And open source GitHub repositories.  We can do this at organizations.  Don't have to be complicated, the simpler the better.
      This is from the American Society of Civil Engineers, it fits on one slide.  Pretty simple.  The important thing is to set appropriate expectations for ethical behavior.  There are loads of other codes of ethics you can use as inspiration, they are very hard to write, I hear.  Read over the different codes, discuss with your colleagues and think about the ethical principles you choose for your own work, your team and your company.  You can use an existing code of ethics or borrow aspects or make your own.
      Once you've chosen an ethical code, communicate it in your team.  Communicating it is up to you.  For example, you can include it in onboarding for starters.  Or add ethical checklists and documentation for new projects.  Or run internal publicity campaigns, maybe like posters on the wall, maybe eyes on top of them.  The important thing is it becomes part of your team and your company culture.
      This act of communicating expectancies is important for empowering team members to speak up if they're uncomfortable before it's too late.  A few years ago, I was working for a consultancy who assigned me to a project for a client I didn't approve of.  But I got so invested in solving the technical aspects of the project, I didn't stop to think about how I was morally working for in client until I was deeply invested.  I moaned about the client to some colleagues, and they told me, if you didn't want to work for this client, that's fine.  But you should have said something at the start of the project.  It made me realize it was okay to say no to client projects, but the appropriate time to do that is before you start work.  The later you leave it, the harder it is to do.
      The next time a dodge you client came along, I felt more comfortable expressing my concerns up front rather than procrastinating for later and we ended up turning down the client.  If we establish policies ahead of time that say it's okay to speak up if you're uncomfortable, we can avoid these kinds of situations.  I tend to think of it as being a little bit like encouraging developers to submit bug reports and point out applications in your processes.  If everyone feels empowered to speak up, then you're all better off.  On a related side note, if you speak up about ethically dubious practices at your place and your employer doesn't listen, you may have a duty to report it to the authorities or otherwise make it public.  Basic dilemma in engineering ethics is the engineer has a duty to the client or employer, but an event greater duty to report a possible risk to others from a client or an employer failing to follow the engineer's instructions.
      A classic example of this is the Challenger space shuttle disaster.  NASA engineers raised warnings about the faulty O rings and the low temperatures on the day of the launch.  But managers ignored the warnings and didn't report to the supervisors.  It was later argued in these circumstances the engineers should circumvent their managers and shout about the dangers until they're heard.  I mentioned ethics checks as a reminder to think about ethical thinking as early as possible.  A friend of mine, a psychotherapist, says they're training ethics as a core part of the process.  Whenever they have a tough decision, there are questions they can use to trigger different types of emotional responses.  Here's a few examples here.  This is quite a monkey brain question.  It's good for triggering emotional reactions like shame.  Is everyone happy with the decision you've made?  For example, if you're lazy about making the site accessible, manual there's a disabled person sitting next to you and would you be comfortable talking about code choices with them?  The second one is more of a consequence response, like a robot brain.  Do you think the consequences are acceptable?  The last reminds me of Emmanuel Kant's categorical imperative, you should only do it  please don't judge me if I've got this wrong, but that's my take.  You should only do something if you're okay with it being universal law.  That's two of the thing.  I think these are a great start.  But feel free to build off of them or tailor them to your own work.  Finally, we can encourage developers to develop more empathy for users to meet them in person.  It's a great way to get them to sit in on user testing sessions.  Empathy for the users helps design better usercentered solutions.  Winwin.
      These ideas are just the start.  They won't fix anything.  They won't put a stop to the fact that a small handful of megacorporations own our digital minds and for profit.  But, you know, one step at a time.  By the way, if you have any questions or suggestions, please come chat to me afterwards or get in touch with me using any of my imaginative online handles.  Thank you.
      [ Applause ]
